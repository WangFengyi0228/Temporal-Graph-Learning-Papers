# 动态图论文

基于发生动态连接后，接受在拓扑结构上直接相连的邻居节点的消息传递，从而聚合动态的邻居节点信息，然后在一个单独的阶段完成时序信息的聚合。

基于随机游走，在对游走进行embedding的过程中聚合时序信息

## 基于游走的方法

### Continuous-Time Dynamic Network Embeddings

WWW18

- 这篇论文提出了一种通用框架，可以非常容易的和现有的节点嵌入方式（基于随机游走）结合，给这些节点嵌入加入时间序列信息。该框架是将时间依赖性纳入现有节点嵌入和基于随机游动的深度图模型的基础（即基于时间序列的随机游走），并且由于保证时序是非递减的，可以减少虚假事件或者不可能的事件来减少噪声。

- 静态图和动态图的区别就在于时间的粒度选择上 动态图尽量选择最小粒度的时间（如秒或者毫秒）来拟合连续的情况，但是如果对用snapshot的方法来看，时间粒度过小将会造成多个snapshot的计算和储存开销过大，该方法利用streaming graph的方式学习，可以用于要求实时性能的应用。

  ![image-20221029190551585](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221029190551585.png)

- ![image-20221029183508895](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221029183508895.png)

  有效的walk由一系列的边连接的节点构成，且边的时间戳必须是递增的。

  ![image-20221029190536733](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221029190536733.png)

- 三种采样游走的边的方法，对于![image-20221029190705156](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221029190705156.png)：

  - 无偏：采样中的每条边被选中的概率都是一样的。

    ![image-20221029190729768](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221029190729768.png)

  - 指数偏置：$t_{min}$是节点连接的边的时间戳的最小值，该分布非常有利于时间较晚出现的边

  - ![image-20221029195607235](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221029195607235.png)

  - 线性偏置：当两条时间连续的边出现大的时间偏差时，将边映射到离散的时间点上。设$η:E->Z^+$是以时间升序方式将数据集中的边进行分类的函数，即$η$将每条边映射为一个索引，对初始边$e$有$η(e)=1$的。在这种情况下，每条边$e$属于$η(E_t)$都对应一个概率：

  - ![image-20221029200411742](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221029200411742.png)

    

- 给定一个时序游走$S_t$，将在STDN中学习包含时序信息的节点嵌入的任务范化为一个最优化问题：![image-20221029201650836](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221029201650836.png)

  其中f是节点嵌入函数。

### Combining Temporal Aspects of Dynamic Networks with Node2Vec for a more Efficient Dynamic Link Prediction

2018 IEEE/ACM

- 把node2vec扩展到动态的连接预测。
- 通过划分数据集在快照留下T个数据集，每个代表一个时间帧。创建一个包含全部节点和边的master graph，使用主图的节点，将节点添加到单独的快照图，这些节点出现在主图而不是快照。这意味着添加的节点会出现在快照，但从网络断开。这使得每一个时序图有相同的节点。
- 利用node2vec采取不同的游走策略（dfs、bfs）来找到对于节点最好的特征表示。

### WalkingTime: Dynamic Graph Embedding Using Temporal-Topological Flows

arxiv2021

- 在过去的四年里，人们对动态网络embedding给予了越来越多的关注，然而现有的动态嵌入方法认为该问题仅限于全局离散状态序列中拓扑结构的演变。本文提出了一种新的嵌入算法WalkingTime，它基于对时间的根本的不同的处理，允许对连续发生的现象进行局部考虑；当其他人认为全局时间步骤是动态环境的一阶citizens时，本文认为由时间上和拓扑上的局部互动组成的流动是我们的基本要素，没有必要对时间相关属性进行任何离散化或调整。

- WalkingTime将node2Vec扩展到动态环境中，采用event-time multi-graph表示，并利用边上的时间信息来inform随机行游走。与之前的工作相比，WalkingTime是基于动态网络模型建立的，不需要全局离散化，允许时间保持为一个连续值，可以在发生交互的节点之间进行局部考虑。此外，该方法允许许多时间信息被原始化使用，消除了以前必须预先做出性能关键决定的几个领域。

- <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221104151800332.png" alt="image-20221104151800332" style="zoom:67%;" />

  在一个无属性、无权重的5*5网格中，将本文的方法与典型的动态图嵌入方法进行比较，本文考虑嵌入橙色的节点3，在图中，可游走到的边显示为黑色，而存在于静态图中但目前未被算法考虑的边为虚线，并以橙色或黄色突出显示。

- WalkingTime将node2Vec延伸到动态图，通过采用一个事件时间multi-graph表示，并利用边的时间信息来影响随机游走。把多条随机游走组合在一起。

### Random walks on temporal networks

2012

- 指出随机行走确实是最简单的扩散模型，其动力学提供了基本的提示来了解整体网络的扩散过程。以随机游走为代表的扩散过程尤其会被拓扑结构上节点之间的路径影响，因此与时序相关的路径代表着动态图的重要特征，因为它们决定着图的动态变化中一系列的因果性交互行为。
- 本文提出了动态图上两个节点之间的两种路径：最快路径和最短路径。

### Evolving network representation learning based on random walks

2020长文

- 本文提出了EVONRL，一种基于随机游走的方法，用于学习演化网络的表征。关键思想是首先在网络的当前状态上获得一组随机游走。然后，当演化中的网络拓扑结构发生变化时，动态地更新备用的随机漫步，这样就不会引入任何偏见。可以利用更新的随机游走集来不断学习从演化中的网络到低维网络表示的准确映射。此外，本文提出了一种分析方法来确定获得不断发展的网络的新表示的正确时间，以平衡准确性和时间性能。进行了全面的实验评估，证明了方法对合理的baseline和不同条件的有效性。
- 本文方法的关键思想是设计有效的方法，以增量的方式更新原始的随机游走集，使其始终考虑到不断发展的网络中发生的变化。因此能够不断学习新的映射函数，从演化中的网络到低维网络表示，只需要更新少量的随机游走来重新获得网络嵌入。这种方法的优点是多方面的：首先，由于网络拓扑结构发生的变化通常是局部的，只有少量的原始随机游走会受到影响，从而产生大量的时间性能收益。此外，由于网络的表征现在将被持续告知，网络挖掘任务的准确度也将得到改善。此外，由于原始的随机游走集被尽可能地保留下来，因此后续的挖掘任务的结果将是相互可比的。

### *STWalk: Learning Trajectory Representations in Temporal

- STWalk是一种无监督的轨迹学习算法，它为给定时间窗口内节点的空间和时间特征生成嵌入向量。
- 当前时刻的图上的游走是space-walk，过去不同时间步的游走被称为time-walk。游走路径被视为sentence，路径上的节点被视为words，输入SkipGram网络用来学习节点轨迹的潜在表征，从而使两个节点在指定窗口大小内共同出现的概率最大化。
- 对于图中的每个节点，将学习两个表征。一个表示对应于当它出现在其他节点的上下文中时的节点嵌入，另一个表示是为节点本身学习的，后者被用作节点的轨迹表征。STWalk考虑的是当前时间和过去时间的节点的影响。

### *evolve2vec: Learning Network Representations Using Temporal Unfolding

AAAI2020

- 提出了一种无监督的动态图学习方法，通过在动态变化的数据集中的同一时间维度上展开随机游走，来综合空间和时间信息，从而生成向量表示。
- evolve2vec具有以下特点：
  - 充足且平衡的结合时序信息，基于采样过程。
  - 没有关于数据集结构的假定。
  - **通过随机游走保留了因果性以及交互的方向性。**
  - 灵活性，位于完全的静态设置和完全的动态设置之间。
  - 并行性，允许可扩展性。
  - 对于潜在连接预测和未来连接预测的效果好于SOTA。

- 为了寻找能同时保留拓扑信息和时序信息的方法，认为过去时间的交互主要影响网络的拓扑结构，最近的交互则对于编码时间信息更为重要。因此将时间窗口<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221102195637686.png" alt="image-20221102195637686" style="zoom: 67%;" />分为两个部分：静态部分<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221102195715361.png" alt="image-20221102195715361" style="zoom:67%;" />和动态部分<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221102195726112.png" alt="image-20221102195726112" style="zoom:67%;" />，通过改变$T_s$可以在完全静态和完全动态之间调整。
- 对静态部分的用户节点采样$c$条长度为$r$的随机游走，动态部分按照快照生成随机游走。

![image-20221102201934834](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221102201934834.png)

​		上图中静态部分开始于A的游走可能是A-C-F或者A-D。动态部分开始于第一个快照中的C点的游走有可能是C-A-A-B-B-D，开始于第二		个快照中的C点的游走有可能是C-E。

- 得到游走后，基于Deepwalk的方法把它们输入到skip-gram，就可以得到融合了拓扑和时序信息的表征结果。

### *A Deep Dive Into Understanding The Random Walk-Based Temporal Graph Learning

- ![image-20221104163344216](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221104163344216.png)

  通过从同一节点进行多次随机行走，可以对网络进行最佳采样。这是因为一次行走只能通过它的一个邻居对一个顶点邻居进行采样,从一个节点进行多次行走可能会对更大的顶点邻域进行采样，从而丰富下游学习任务的信息量。上图(b)显示了从一个节点进行多次随机行走对链接预测和节点分类的预测精度的影响。该图证实了从同一节点进行更多的行走会增加预测的准确性。有趣的是，这种改善在8-10次行走后达到了饱和。这是因为现实世界图的幂律性质，即大多数节点的邻居很少。在大多数稀疏连接的节点中，执行8-10次行走就足以覆盖大多数邻居。除此以外，进行更多的行走，其价值是有限的。

  随机漫步的长度表示被采样的邻居与源的距离。例如，长度为5的随机漫步将从源顶点采样一个5跳的邻居。虽然每个节点的多个随机游走都会对广泛的邻域进行采样，但较大的随机游走长度表明了采样的邻域深度。直观地说，随机行走的长度越大，网络可以被采样的深度就越深。上图(c)显示，随着随机行走长度的增加，预测精度也在增加。然而，这一趋势在走动长度达到4-6后趋于饱和，这可以用前述的发现来描述。下图显示，随机行走的频率随着行走长度的增加而减少。这就转化为大步行长度的边际信息增益和预测精度的饱和。

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221104164441144.png" alt="image-20221104164441144" style="zoom:67%;" />

  在高层次上，图学习任务将每个节点映射到一个嵌入空间，其中嵌入空间的维度定义了可以建模的交互的复杂性。虽然之前的算法工作使用了128的固定维度大小，但本文分析了这是如何影响端到端的准确性。上图(d)显示了改变嵌入维度对预测精度的影响。将d从1增加到8的结果是预测精度的提高，因为更高的维度可以模拟更复杂的网络互动。有趣的是，我们发现，8维的嵌入空间足以做出有意义的网络预测。

  总而言之，在算法性能和运行时间复杂性之间存在着丰富的权衡空间。虽然增加上述超参数的值会单调地增加不同内核的执行时间，但它们对预测精度的影响是有限的。基于经验发现，每个节点的随机行走数、随机行走长度和嵌入空间维度的最佳值分别为10、6和8。

### Towards Real-Time Temporal Graph Learning

arxiv2022

- 之前的图表征学习工作是在预先收集的时间图数据上操作的，并不是为了处理图上的实时更新。现实世界的图是动态变化的，其整个时间上的更新是无法预先获得的。本文提出了一个端到端的图学习pipeline，它可以执行动态图构建，创建低维节点嵌入，并在在线环境下训练多层神经网络模型。神经网络模型的训练被认为是主要的性能瓶颈，因为它在许多顺序连接的低维内核上执行重复的矩阵操作。本文建议释放这些低维核中的细粒度并行性以提高模型训练的性能。

- 本文的主要目的是实现和描述一个时间上更新的图学习pipeline，并使用在线设置进行FNN训练，其中训练数据随着图的演变而变化。我们的pipeline在每个时间戳t进行时间图快照，并从基于R-Tree的图构建步骤开始，该步骤可以跟踪自最后一个时间戳t-1以来图中的更新。然后，这些更新被流向GRL步骤，该步骤在图上执行随机行走和word2vec以获得节点嵌入。利用EvoNRL从时间上更新的图中不断学习嵌入到低维图表示中，同时避免图快照之间的冗余更新。然后，图的更新被转发给最后的训练步骤，用于链接预测或节点分类。采用了一个在线学习策略，在每个时间戳t的连续迭代中训练一个单一的时态图批次，这些步骤在随后的时间戳{t + 1, t + 2, ..., N }中重复。

- 执行时间主要取决于FNN训练阶段，虽然在随机游走和word2vec提出了几种并行化方法，但FNN训练的实现被遗漏了。本文探讨了FNN训练的细粒度并行化以实现性能加速。考虑了FNN模型每个迭代的前向和后向传播路径中各个矩阵核的并行化机会，为FNN模型中的每个低维内核实现了四种最先进的矩阵乘法算法（即内乘、外乘、行乘和列乘积）。在大核数共享内存多核上的评估表明，使用正确的并行化策略可以产生显著的性能扩展潜力。

- ![image-20221104144358079](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221104144358079.png)

  该pipeline在每个时间戳t获取时间上变化的图的快照，并使用图的构造步骤处理时间上的更新。然后，这些更新被发送到随机游走和word2vec步骤，将图快照映射到低维嵌入空间。最后，更新的图嵌入被送入FNN训练步骤，用于链接预测或节点分类。

- ![image-20221104145422597](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221104145422597.png)

  用R-Trees构建图的概述，它将所有在MBR中相互链接的节点聚在一起，并且在图随时间演变时只更新树结构的一部分，以避免冗余更新。

  在时间设置中学习节点嵌入时，只应该考虑图的新的更新。出于这个原因，这种基于R-Tree的图构建方法是一种合适的技术，可以跟踪时间戳之间的节点交互。在这一步骤中，当时间戳t的时间图数据流输入时，所有相互之间形成边的节点都被聚拢在最小边界矩形（MBR）中，并以树状结构表示。虽然一些节点随着时间的推移而改变它们的相互作用，但并不是每个节点在每个时间戳都改变其相互作用。因此，不是处理每一个节点，而是只更新包含改变了其相互作用的节点的MBRs。只要节点仍在其MBR中，就不需要更新树状结构。

- ![image-20221104150026398](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221104150026398.png)

  当发生新的连接(u,v)后，并不产生全新的random walk，而是对上一个randomwalk中包含u、v的地方进行更新。

  ![image-20221104150111273](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221104150111273.png)

### Time-aware Random Walk Diffusion to Improve Dynamic Graph Learning

2022 arxiv

- 一些图增强方法没有考虑到动态性，例如最近发生的边比早期的边更具影响力。本文提出了方法TIARA(Time-aware Random Walk Diffusion)，一种基于扩散的方法，用于增强以图形快照的离散时间序列表示的动态图。为此首先设计了一个时间感知的随机行走的proximity，以便surfer可以沿着时间维度以及边缘行走，从而得到空间和时间上的本地分数。然后，根据时间感知的随机行走推导出扩散矩阵，并将它们表明为增强的邻接矩阵，空间和时间的局部性都得到了增强。通过大量的实验证明了TIARA有效地增强了一个给定的动态图，并使得各种图数据集和任务的动态GNN模型的显著改善。

- 本文的目标是通过增加输入数据来提高动态GNN的性能。

  ![image-20221103163705803](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221103163705803.png)

  即对于给定的动态图的邻接矩阵序列，生成新的邻接矩阵序列来提升模型性能。

  ![image-20221103164232057](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221103164232057.png)

## GNN和RNN融合

### GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs

2018

- 本文提出了一种新的网络结构，门控注意力网络（GaAN）用于在图上学习。与传统的多头注意机制不同的是，GaAN使用卷积子网络来控制每个注意头的重要性。我们在归纳节点分类问题上证明了GaAN的有效性。此外，以GaAN为构建模块，构建了图门控递归单元（GGRU）来解决交通速度预测问题。在三个真实世界的数据集上进行的广泛实验表明，GaAN框架在这两项任务上都取得了最先进的结果。

- 最近的研究已经转向通过图卷积来解决如何找到正确的方式来表达和利用图的基本结构信息，它将规则网格拓扑上的卷积的标准定义概括为图结构上的 "卷积"。图卷积的基本思想是在一组相邻的节点上开发一个局部的参数共享算子，以聚合一组局部的低级特征。本文把这样的算子称为图聚合器，把本地节点的集合称为聚合器的接受域。然后，通过堆叠多个图聚合器，本文建立一个深度神经网络模型，该模型可以进行端到端的训练，以提取整个图的局部和全局特征。本文使用空间定义而不是图卷积的谱定义，因为完整的谱域处理需要对拉普拉斯矩阵进行eigendecomposition，这在大图上是难以计算的，而local版本可以被解释为图聚合器。

- 图聚合器是图卷积神经网络的基本构建模块，一个模型捕捉图的结构信息的能力主要由其聚合器的设计决定，大多数现有的图聚合器都是基于对邻域的汇集或计算邻域特征的加权和。从本质上讲，**具有偏移不变性（permutation invariant）并且可以动态调整大小的函数是合格的图聚合器**。这类函数中的一类是神经注意网络，它使用一个子网络来计算一个集合中元素的相关权重。在注意力模型家族中，多头注意力模型已被证明对机器翻译任务有效，后来它被采用为图聚合器来解决节点分类问题，一个注意力头将与查询向量相似的元素汇总到一个表示子空间中。使用多个注意力头可以探索不同表示子空间中的特征，这可以提供更多的自然界建模能力。然而，平等对待每个注意头，就失去了从某些注意头中获益的机会，因为这些注意头在本质上比其他注意头更重要。

  为此本文提出了用于图上学习的门控注意力网络（GaAN），GaAN使用一个小型卷积子网络来计算每个注意头的soft gate，以控制其重要性。与传统的多头注意力不同的是，门控注意力可以通过引入的门来调节所关注内容的数量。本文通过将其应用于归纳节点分类问题来证明新聚合器的有效性，还改进了采样策略以减少内存成本并提高运行时间效率，以便在相对较大的图上训练模型和其他图聚合器。此外，由于提出的聚合器非常通用，将其扩展到构建一个图门控递归单元（GGRU），这直接适用于时空预测问题。在两个节点分类数据集PPI和Reddit和一个交通速度预测数据集METR-LA上进行的广泛实验表明，GaAN始终优于基线模型，并达到了最先进的性能。

- 图聚合器的通用表述：给定一个节点$i$和它的邻居节点$N_i$，图聚合器是一个函数$γ$，其形式为$y_i = γ_Θ(xi, {z_{N_i}})$，其中$x_i$和$y_i$是中心节点$i$的输入和输出向量。 $z_{N_i} =  \{z_j|j∈N_i\}$是邻居节点中的参考向量集合，$Θ$是聚合器的可学习参数。在本文中不考虑使用边缘特征的聚合器。然而，通过定义$z_j$包含边缘特征向量$e_{i,j}$，将边缘纳入本文的定义中是很简单的。

- **MULTI-HEAD ATTENTION AGGREGATOR**

  ![image-20221118210726252](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221118210726252.png)

  qi中$K$是注意头的数量。$w^{(k)} _{i,j}$是中心节点$i$和邻居节点$j$之间的第$k$个注意权重，它是通过对点积值应用softmax产生的。$θ^{(k)}_{xa}$，$θ^{(k)}_{za}$和$θ^{(k)}_{v}$是第$k$个头的参数，用于计算Q、K和V向量，其维度分别为da、da和dv。K个头的输出与输入向量相连接，并传递给一个以$θ_o$为参数的输出全连接层，得到最终输出yi，其维度为do。该聚合器与GAT的区别在于，采用了键值注意机制和点乘注意，而GAT不计算额外的值向量，并使用全连接层来计算$φ^{(k)}_w$。

- **GATED ATTENTION AGGREGATOR**

  ![image-20221118211645609](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221118211645609.png)

  虽然多头注意聚合器有能力探索中心节点和其邻域之间的多个表征子空间，但并非所有这些子空间都同样重要；某些子空间甚至可能对某些节点不存在，输送捕获无用表征的注意力头的输出会误导模型的最终预测。因此，本文在0（低重要性）和1（高重要性）之间计算一个额外的soft gate，为每个头分配不同的重要性，结合多头注意力聚合器得到门控注意力聚合器的表述：

  ![image-20221118211316176](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221118211316176.png)

  其中$g^{(k)}_i$是一个标量，即节点i的第k个头的门值。为了确保增加门值不会引入太多额外的参数，使用卷积网络$ψ_g$，取中心节点和邻近节点的特征来生成门值。$ψ_g$网络有多种可能的设计，在本文中结合平均池化和最大池化来构建网络，详细公式如下：

  ![image-20221118211511672](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221118211511672.png)

  这里，$θ_m$在取元素最大值之前将邻居特征映射到$d_m$维的向量，$θ_g$将串联的特征映射到最终的K门。通过设置一个小的$d_m$，计算门的子网络将有可忽略的计算开销。

  ![image-20221118211657113](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221118211657113.png)

### Continuous-Time Dynamic Graph Learning via Neural Interaction Processes

CIKM2020

- 动态图建模的一个主要挑战是如何有效地将时间和结构信息编码到非线性和紧凑的动态嵌入中。为了实现这一目标，本文提出了一种基于原理性图神经网络的方法来学习连续时间动态嵌入。首先定义了一个时间依赖性交互图（TDIG），它是由交互数据的序列诱导出的，基于TDIG的拓扑结构开发了一个名为TDIG-MPNN的动态消息传递神经网络，它可以捕获TDIG的细粒度的全球和局部信息。此外，为了提高连续时间动态嵌入的质量，在上述TDIG-MPNN层之前应用了一种新颖的选择机制，包括两个连续的步骤，即co-attention和gating，通过考虑互动节点在k跳邻居之间的高阶关联性来调整节点的重要性。最后，将学习问题放在时间点过程（TPPs）的框架中，使用TDIG-MPNN为动态交互过程设计一个神经强度函数。模型在多个数据集的时间交互预测（包括tranductive和inductive）上取得了优于其他方法的性能。
- Temporal point processes (TPPs) 将事件时间（或时间间隔）视为随机变量的时间点过程，提供了一种优雅的方式来模拟细粒度的动态。
- 本文的工作属于通过TPPs框架学习连续时间动态网络嵌入的范畴，提出了一个强大的基于图神经的方法来学习时间演化图上的信息嵌入，从中设计一个神经强度函数来模拟互动的发生率。
  - 将时间演化的动态图正式定义为由一连串的时间级联互动所引起的时间依赖互动图（TDIG），它构成了之后建模的基础。与使用快照对时间演化图进行粗略的近似相比，本文的处理方法保持了细粒度的时间拓扑学信息；与DeepCoevol和JODIE的处理方法不同，本文考虑了所有相关的互动节点来构建图，并保持了更精细的演化信息。
  - 根据TDIG的拓扑结构，进一步提出了一个新的深度学习架构来进行表征学习，其中一个动态消息传递神经网络（TDIG-MPNN）被赋予计算动态嵌入。具体来说，**TDIG-MPNN包括一个异质图注意（TDIG-HGAN）网络和一个循环图神经网络（TDIG-RGNN），它可以捕获局部和全局图信息**，从而生成信息丰富的节点表示。
  - 受历史上的相互作用（节点）也会因其不同的内容而具有不同的重要性这一事实的启发，提出了一个应用于TDIG-MPNN之前的插件模块，名为选择机制，它允许网络有选择地强调有信息的节点并抑制不相关的节点。具体来说，首先对交互式节点的k深度邻居（历史）的TDIG进行co-attention操作以捕获高阶相关性，然后在此基础上学习内容偏移门函数以调整节点的重要性。据我们所知，这是第一次尝试在学习连续时间图嵌入时探索高阶接近性以提高动态表示的质量。
  - 基于学到的动态嵌入，设计了一个隐含的神经强度函数，该函数探索了非线性的加法和乘法交互关系，然后采用它来描述我们在TDIG的神经交互过程。本文的方法还可以纳入节点和互动的属性，使其对未见过的节点及其互动进行归纳。

- ![image-20221118172534617](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221118172534617.png)

  一个由五个时间上的相互作用组成的序列和相应的TDIG在$t_5$的例子。(a)黑色箭头表示相邻的相关交互之间的依赖关系，双向的紫色虚线箭头表示交互边。所有的边都是有方向的，代表信息在TDIG上的传递方式。(b)  在时间$t_6$观察到一个新的交互$(u,v,t_6)$后，相应的TDIG被更新为一个新添加的双向交互边$l_{u,v,t_6}$和四个新添加的定向依赖边，即$e^D_0$、$e^D_1$、$e^D_2$ 和 $e^D_3$。红色虚线表示节点𝑢在$t_6$处的3深度TDIG子图$G_{t_6}(𝑢, 𝑘 =  3)$，$Δ(v,t_6)$代表$e^D_3$和$e^D_4$之间的时间间隔(作为边$e^D_3$和$e^D_4$的边特征)，其中节点𝑣参与了最后一次交互  "Lucy在$t_5$看到骷髅电影"。

- **TDIG-MPNN**：本文的工作旨在学习连续时间动态嵌入，它可以很好地编码非线性和细粒度的时间和结构信息。受一般GNN框架的启发，提出了名为TDIG-MPNN的动态图信息传递神经网络，它通过聚合节点邻域的信息来计算节点的嵌入。TDIG-MPNN是一个由TDIG-informed异质图注意网络（TDIG-HGAN）和TDIG-informed递归图神经网络（TDIG-RGNN）组成的联合模型。TDIG-HGAN通过收集TDIG上节点邻域的信息，考虑了局部的时间和结构特征，然后TDIG-RGNN通过时间上的递归进一步整合TDIG-HGAN的输出，捕捉TDIG的**全局（长期依赖关系）**信息。通过这种方式，所提出的TDIG-MPN可以为每个节点生成一个信息丰富的动态嵌入，捕捉到局部和全局的图信息。

### K-Core Based Temporal Graph Convolutional Network for Dynamic Graphs

- 图表示学习是各种应用中的一项基本任务，它致力于学习能够保留图形拓扑信息的节点的低维嵌入。然而，许多现有的方法专注于静态图，而忽略了不断变化的图的模式。受图卷积网络(GCNs)在静态图嵌入中的成功启发，本文提出了一种新型的基于k核的时序图卷积网络，即CTGCN，用于学习动态图的节点表示。与之前的动态图嵌入方法相比，**CTGCN可以同时保留局部连接接近性和全局结构相似性**，并同时捕捉图的动态变化。在提出的框架中，传统的图卷积被概括为两个阶段，即特征转换和特征聚合，这使CTGCN具有更多的灵活性，并使CTGCN能够在同一框架下学习连接性和结构性信息。在7个真实世界的图上的实验结果表明，CTGCN在一些任务中的表现优于现有的最先进的图嵌入方法，包括链接预测和结构角色分类。

- 由于GCNs和相关方法在保留全局图结构方面的表现力较差，这些方法可能会遗漏图中一些潜在的重要信息，如下图所示，在挖掘潜在的金融欺诈时，连接接近性和结构相似性都很重要。换句话说，连接接近性可以帮助揭示图中的金融欺诈团伙，如黑色虚线矩形中的人，而结构相似性可以用来识别具有类似异常社会行为的金融欺诈者，如红色虚线圆圈中突出的人。

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121133216225.png" alt="image-20221121133216225" style="zoom:80%;" />

  尽管一些动态图方法可以捕获时间上的演变模式，但它们在保留图中丰富的结构信息方面能力有限。因此，如何有效地捕捉图的动态变化并保留结构信息是至关重要的，目前仍未得到解决。

- **Graph convolutional networks**

  一个vanilla GCN中的图卷积层可以定义为：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121133635507.png" alt="image-20221121133635507" style="zoom:80%;" />

  其中$X$是输入特征矩阵，<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121133714367.png" alt="image-20221121133714367" style="zoom: 80%;" />是具有自环的邻接矩阵，<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121133737056.png" alt="image-20221121133737056" style="zoom:80%;" />是一个对角矩阵：<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121133806388.png" alt="image-20221121133806388" style="zoom:80%;" />，$W$是可学习的权重矩阵，<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121133839738.png" alt="image-20221121133839738" style="zoom:80%;" />是一个非线性激活函数，$H$是输出的嵌入矩阵。显然，图卷积层与全连接层类似，只是扩散矩阵<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121134139956.png" alt="image-20221121134139956" style="zoom:80%;" />将一个节点的邻居的特征传播给这个节点。实质上，图卷积是拉普拉斯平滑的一种特殊形式，它将一个节点的新特征计算为其自身及其邻居的加权平均值。一层GCN可以保留一阶接近性，多层GCN在实践中经常被利用，因为它可以捕获高阶接近性信息。然而，由于固有的拉普拉斯平滑特性，当图形卷积层增加时，GCN会出现过度平滑和过度拟合，这些问题反映了GCN无法探索全局图结构。因此，在实践中，一个GCN通常最多包括3个图卷积层。

  受上述图卷积操作的启发，本文将该框架概括为两个阶段：特征转换和特征聚合：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121134455086.png" alt="image-20221121134455086" style="zoom:80%;" />

  其中$X$是输入特征矩阵，$f$是将$X$映射到隐藏空间的特征转换函数，$A$是邻接矩阵，$g$是$A$的函数，定义了传播规则，$h$是特征聚合函数，根据传播规则$g$聚合每个节点的特征，$H$是嵌入矩阵，整合了特征转换和图结构的信息。许多以前的工作都可以包括在这个框架中。

- 本文提出了一种新型的基于k-core的时序图卷积网络，即CTGCN，以保持动态图中的连接接近性和结构相似性。如下图所示，CTGCN包括两个模块：一个基于核的GCN模块和一个图演化模块。这也是一个广义的GCN框架的实例，虽然CTGCN是一种时序GCN方法，但可以把h看作是一个复杂的时空特征聚合函数。

  ![image-20221121134354406](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121134354406.png)

  **Core-Based GCN Module**：为了捕捉丰富的图结构信息，利用kcore子图来设计一个基于核的GCN，即CGCN。k-core定义：考虑一个图$G$和图$G$的一个子图$G_0$，如果$G_0$是$G$的一个最大子图，其中所有节点的度数至少为$k$，则$G_0$是$G$的k-core。从该定义可以推断出一些有趣的k-core属性：其中一个属性是，在现实世界的图中，节点的k-core数与节点度数有很强的正相关。另一个属性是k核形成一个嵌套结构。形式上，让<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121140116303.png" alt="image-20221121140116303" style="zoom:80%;" />是图$G$的k-core集；那么，$C$中的所有k-core形成一个嵌套链：

  ![image-20221121140249214](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121140249214.png)

  其中$k_{max}$是$G$的最大k-core数。所有k-core可以通过k-core分解算法提取，其时间复杂度与边缘数呈线性关系，在处理大规模图时很有效率。值得注意的是，k核分解可以被看作是一种图的分割策略，类似的图分割策略在ClusterGCN中也得到了利用。ClusterGCN通过图聚类算法将一个图分割成几个子图，以限制GCN层中的邻居扩展，从而防止GCN中的过度平滑问题。然而，本文的目的是增强GCN的属性，使CTGCN不仅能保留局部的连接接近性，还能捕捉动态图中的全局结构信息。

  由于$k$核自然地定义了一个分层嵌套的子图结构，所有的$k$核都可以被利用来传播图中不同尺度的节点特征。基于核的图卷积层的定义如下图所示。令<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121140529534.png" alt="image-20221121140529534" style="zoom:80%;" />是特征转换矩阵，则每个$k$核子图$C_k$上的特征传播定义为：<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121140702673.png" alt="image-20221121140702673" style="zoom:80%;" />其中<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121140718137.png" alt="image-20221121140718137" style="zoom:80%;" />是$k$核子图的扩展邻接矩阵，增加了所有节点的自连接，以使所有$k$核邻接矩阵具有相同的维度。

  在$k$核子图上进行特征传播后，获得更具表现力的特征矩阵的直观方法是通过聚合函数（如SUM）聚合所有$Z'_k$。此外，假设每个图都有一个潜在的k-core子图演化过程。递归神经网络（RNN），如门控递归单元（GRU）或长短时记忆（LSTM），**可以被用来捕捉潜在k-core子图演化过程中的复杂子图演化模式**，其定义为：<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121141114969.png" alt="image-20221121141114969" style="zoom:80%;" />其中<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121141125878.png" alt="image-20221121141125878" style="zoom:80%;" />是一个零矩阵。CGCN层的输出由以下公式给出:

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121141140478.png" alt="image-20221121141140478" style="zoom:80%;" />

  如图所示，特征转换矩阵Z在所有RNN单元中共享，这减少了可学习的权重矩阵的数量。注意到<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121141213886.png" alt="image-20221121141213886" style="zoom:80%;" />包含在了<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121141226589.png" alt="image-20221121141226589" style="zoom:80%;" />。因此，变化矩阵<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121141249247.png" alt="image-20221121141249247" style="zoom:80%;" />被用于稀疏矩阵乘法，这减少了内存成本并提高了计算效率。与vanilla  GCN层类似，CGCN层只保留了一阶接近性，多个CGCN层有助于在更大的感受野中捕捉连接性信息。

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121141341410.png" alt="image-20221121141341410" style="zoom:80%;" />

​	**Graph Evolving Module**

- 为了有效地捕捉图进化过程中的图动态，本文利用RNN，如GRU或LSTM，来模拟CGCNs在不同时间戳的节点表示之间的时间依赖关系。请注意，如果需要更复杂的时间特征，这个模块也可以由其他RNN架构来增强，也可以考虑不同时间戳的$k$核子图的时间依赖性，但这将影响CTGCN的计算效率。形式上，图演化模块定义如下：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121141708414.png" alt="image-20221121141708414" style="zoom:80%;" />

  其中<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121141723524.png" alt="image-20221121141723524" style="zoom:80%;" />是一个零矩阵，<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121141747980.png" alt="image-20221121141747980" style="zoom:80%;" />是$G_t$的邻接矩阵，$X_{(t)}$是$G_t$的节点特征矩阵，$t=1,...,T$。**每个CGCN为RNN单元生成静态节点嵌入，RNN单元决定在下一个时间戳的层中应该保留多少信息。**

### Variational Graph Recurrent Neural Networks

(NIPS2019)

- 本文开发了一个新的分层变异模型，引入了额外的潜伏随机变量来共同模拟图循环神经网络（GRNN）的隐藏状态，以捕捉动态图中的拓扑结构和节点属性变化。本文认为，在这个变异的GRNN（VGRNN）中使用高水平的潜伏随机变量可以更好地捕捉动态图中观察到的潜在变异性以及节点潜伏表示的不确定性。通过为这个新的VGRNN架构开发的半隐式变异推理（SIVGRNN），表明灵活的非高斯潜表征可以进一步帮助动态图分析任务。在多个真实世界的动态图数据集上的实验表明，SI-VGRNN和VGRNN在动态链接预测方面一直比现有的基线和最先进的方法要好得多。

- 图卷积递归网络（GCRN）用于对静态图的节点上定义的时间序列数据进行建模。视频中的帧系列和传感器网络上的时空测量是这类数据集的两个例子。**GCRN结合了图卷积网络（GCN）和递归神经网络（RNN）来捕捉数据的空间和时间模式。**更确切地说，给定一个有$N$个节点的图$G$，其拓扑结构由邻接矩阵$A∈R^{N×N}$决定，节点属性序列$X={X(1), X(2), . . , X(T )}$，GCRN读取$M$维节点属性$X(t)∈R^{N×M}$，并在每个时间步骤$t$更新其隐藏状态$h_t∈R^p$：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20230212145434588.png" alt="image-20230212145434588" style="zoom:67%;" />

- 这里f是一个非概率的深度神经网络。它可以是任何递归网络，包括门控激活函数，如长短期记忆（LSTM）或门控递归单元（GRU），其中它们内部的深层被图卷积层所取代。GCRN通过参数化联合概率分布的因子化作为条件概率的乘积来模拟节点属性序列：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20230212150103098.png" alt="image-20230212150103098" style="zoom:67%;" />

  由于过渡函数f的确定性，这里的映射函数g的选择有效地定义了联合概率分布：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20230212150125245.png" alt="image-20230212150125245" style="zoom:67%;" />

  可以用标准GCRN表示。对于高度可变的序列来说，当X的可变性很高时，模型试图将这种可变性映射到隐藏状态h中，导致h的潜在高变化，从而导致训练数据的过拟合。因此，GCRN并不完全能够对具有高变化的序列进行建模。自回归模型的这一基本问题已经通过在模型中引入随机的隐藏状态来解决非图形结构的数据集。

- 本文将GCN和RNN整合到一个图RNN（GRNN）框架中，这是一个动态图自动编码器模型。GCRN旨在对静态图上定义的动态节点属性进行建模，而**GRNN可以在不同的时间快照下得到不同的邻接矩阵，并通过对隐藏状态$h_t$采用内积解码器来重构时间t的图。**更确切地说，ht可以被看作是时间t的动态图的节点嵌入。为了进一步提高GRNN的表达能力，通过将GRNN与异质图自动编码器（VGAE）相结合，引入随机潜变量。这样一来，不仅可以在不做平滑性假设的情况下捕捉图之间的时间依赖性，而且每个节点都用潜空间中的分布来表示。此外，VGRNN中设计的先验结构使其能够预测未来的时间步骤中的链接。



### Structured Sequence Modeling with Graph Convolutional Recurrent Networks

### GC-LSTM: Graph Convolution Embedded LSTM for Dynamic Link Prediction



### DynaGraph: Dynamic Graph Neural Networks at Scale

- 本文提出了DynaGraph，一个能有效支持动态图谱神经网络（GNN）的系统。基于现有的动态GNN架构建议将结构和时间信息编码技术独立地结合起来的观察，DynaGraph提出了新的技术，使这些任务的交叉优化成为可能。它使用缓存的消息传递和时间步长的融合来大大减少与动态GNN处理相关的开销。它进一步提出了一个简单的分布式数据并行动态图处理策略，实现了可扩展的动态GNN计算。

- 本文专注于高效的动态GNN训练。支持动态图的自然方法是将其视为一系列静态快照，正如现有的时间演化图处理系统所提出的那样。虽然时间演化图处理系统的主要任务是在一系列图的快照上执行图算法，但动态GNN还有一个额外的任务是捕捉时间上的依赖关系。因此，动态GNN的一般方法是将利用图中结构信息的空间嵌入技术与时间信息编码技术相结合，在生成的嵌入中捕捉动态方面的信息。空间嵌入由静态GNN技术实现，如GCNs和GAT，而捕捉时间编码的常见方法是使用循环神经网络（RNN），如LSTMs或GRU。因此，现有的动态GNN提案通过交替堆叠GNN和RNN，整合GNN和RNN操作，或使用自动编码或生成模型来结合GNN和RNN。不幸的是，所有这些方法都会导致显著的性能问题，并产生可扩展性瓶颈。
- 本文提出了DynaGraph，一个能够实现高效动态GNN训练的系统。DynaGraph是基于这样的观察：现有的动态GNN提案中效率低下的根源在于它们独立地结合了结构和时间的嵌入操作。因此，它们错过了优化许多基础操作的机会。因此，DynaGraph专注于通过利用空间和时间嵌入生成的计算结构来减少或消除这种低效率，并利用机会在它们之间利用这种知识。DynaGraph提出了三种技术来提供动态GNN的高效训练：
  - 首先，DynaGraph提出了缓存信息传递，作为一种消除冗余邻域聚合的技术。这是基于我们的观察，结合GNN和RNN的流行方法是通过使用图卷积操作取代RNN中的矩阵乘法。我们把这种组合表示为GraphRNN单元。因此，每个GraphRNN单元将由多个GNN操作组成。例如，一个基于LSTM的动态GNN（GraphLSTM）单元由8个GNN操作组成。虽然GraphRNN的每个门摄入两种操作--与时间有关的和与时间无关的，但每一种都在不同的输入上独立执行邻域聚合。然而，输入在门之间共享或部分共享，并在一个时间步长的同一快照上执行邻域聚合。例如，GraghLSTM的四个门都将节点特征作为输入，用于与时间相关的GNN操作，并将前一时间段的hiddens用于与时间相关的GNN操作。DynaGraph使用这种跨操作信息来建立缓存信息传递，图实体之间的信息在目的地被缓存并重复使用。为了识别这种消息的冗余，DynaGraph使用了一个简单的接口，唯一地识别跨空间和时间操作的消息，并将其缓存起来供将来使用。然后，它使用众所周知的PUT和GET接口将缓存公开。
  - 第二，它使用时间步长的融合来减少GPU对小型真实世界数据集的利用率不足。我们这样做是基于我们的观察，现有的各种应用的动态图通常有少量的输入特征，往往图中的节点与少于10个的特征相关。此外，空间-时间图，其中节点特征随时间变化，而底层图结构是静态的，被广泛用于各种应用，如交通预测和流行病预测。由于图结构是静态的，与时间无关的GNN在不同时间段的邻域聚合可以融合为一个，以节省计算量。DynaGraph利用这一信息，融合了动态GNN中与时间无关的操作的输入，从而减少了冗余的邻域聚合，避免了GPU的利用率不足。
  - 第三，DynaGraph为分布式环境下的动态GNN训练提出了一个数据并行的执行模型。我们观察到，现有的支持分布式GNN训练的框架，如DGL，使用单一图（快照）上的图分区作为主要技术，在机器上分配工作。然而，动态GNN通常有大量的图的快照，它们在那里以序列的方式被训练。将同样的方法扩展到分配动态GNN，由于分区和通信开销，导致性能不佳。相反，DynaGraph建议在集群中的GPU上以序列级别对快照进行划分。然后，它以数据并行的方式执行GNN-RNN操作，从而提供高效的分布式训练。

### ROLAND: Graph Learning Framework for Dynamic Graphs

（KDD2022 DTDG）

### dyngraph2vec: Capturing Network Dynamics using Dynamic Graph Representation Learning

DTDG

## 不基于游走的方法

### Learning Graph Dynamics using Deep Neural Networks

2018

- ![image-20221110172041922](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221110172041922.png)

- 将GNN嵌入RNN，LSTM网络的输入层有MN个输入神经元，来自CNN的输出被送入它。来自RgCNN框架的输出要经过一个全连接的密集层来进行分类和预测。$x_t^{gCNN}$是gCNN的输出。

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221110172550571.png" alt="image-20221110172550571" style="zoom:67%;" />



### DynGEM: Deep Embedding Method for Dynamic Graphs

2018DTDG

- 把大型图映射为低维空间中的嵌入，由于其广泛的应用，如图的可视化、链接预测和节点分类，最近引起了极大的兴趣。现有的方法主要是计算静态图的嵌入，然而实际应用中的许多图是动态的，并随着时间的推移不断演变。对动态图的每个快照独立地应用现有的嵌入算法，通常会导致在稳定性、灵活性和效率方面不令人满意的表现。这项工作提出了一个高效的算法DynGEM，该算法基于用于图嵌入的深度自动编码器的最新进展，来解决这些问题。DynGEM的主要优点包括：(1) 嵌入在一段时间内是稳定的，(2) 它可以处理不断增长的动态图，以及(3)  它比在动态图的每个快照上使用静态嵌入方法具有更好的运行时间。在各种任务上测试了DynGEM，包括图的可视化、图的重建、链接预测和异常检测（在合成和真实数据集上）。实验结果表明，该方法具有优越的稳定性和可扩展性。
- 当前动态图的处理方法通常是对于动态图的每一个快照直接利用静态图嵌入方法进行表示学习然后按对于每一个快照的结果按时间进旋转对齐。直接将静态图方法应用于动态图存在着以下挑战：
  - 稳定性：静态方法生成的嵌入表示是不稳定的，在图没有很大变化的情况下，图的嵌入表示在连续时间步内会有很大不同。
  - 图的增长：随着时间的演化，图中会增加新的节点，同时增加新节点到现有节点的连接。而当前的的图嵌入技术都是假定图的节点是固定的，因此无法处理这种演化图。
  - 可扩展性：对于动态图的每个快照独立的进行嵌入学习的方法的运行时间与动态图的快照是线性关系。当前间个图嵌入学习的计算代价就已经很昂贵，因此，这种方法并不能很好的扩展到动态图中。

- DynGEM方法的核心是一个深度自编码器，并采用深度学习方法生成高度非线性的嵌入表示。DynGEM并不是从头开始学习每个图快照的嵌入表示，而是基于前一个图快照的嵌入表示增量的生成当前时刻的图快照的嵌入表示。具体来说，以前一个时刻的图快照的嵌入表示为初始值，执行梯度训练。这种方法不仅确保了嵌入在时间上的稳定性，而且由于在第一个时间步之后的所有嵌入只需要很少的迭代就能收敛，因此可以实现高效的训练。为了处理节点数量不断增加的动态图，本文使用启发式PropSize逐步增加神经网络的大小，以动态确定每个快照所需的隐藏单元数量。除了所提出的模型，本文还为动态图嵌入引入了严格的稳定性度量。

- 对于动态图快照$G=\{G_1,G_2,...,G_T\}$，其中$G_t=(V_t,E_t)$，考虑增长图的设置，即$V_t⊆  V_{t+1}$，即新节点可以加入动态图并创建到现有节点的链接，将删除的节点视为图的一部分，其余节点的权重为零。假设$E_t$和$E_\{t+1\}$之间没有关系，快照之间可以形成新的边，而现有边可以消失。动态图嵌入将嵌入的概念扩展到动态图，给定动态图$G=\{G_1,G_2,...,G_T\}$，动态图嵌入是映射$F=\{f_1,f_2,...,f_T\}$的时间序列，使得映射$f_T$是$G_T$的图嵌入，并且所有映射保留其各自图的接近度。一个成功的动态图嵌入算法应该随着时间的推移创建稳定的嵌入。直观地说，稳定的动态嵌入是这样的一种嵌入：如果基础图变化很小，则连续嵌入仅相差很小的量，即如果$G_{t+1}$与$G_t$相差不大，则嵌入输出$Y_{t+1}＝f_{t+1}(G_{t+1})$和$Y_t＝f_t(G_t)$也变化很小。更具体地说，设<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221122104148563.png" alt="image-20221122104148563" style="zoom:80%;" />是节点集的诱导子图<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221122104235906.png" alt="image-20221122104235906" style="zoom:67%;" />的加权邻接矩阵和<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221122104321121.png" alt="image-20221122104321121" style="zoom:67%;" />是所有在<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221122104235906.png" alt="image-20221122104235906" style="zoom:67%;" />中的节点$V$的在时间$t$的快照的嵌入，将绝对稳定性定义为：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221122104500309.png" alt="image-20221122104500309" style="zoom:80%;" />

  任何嵌入$F$的绝对稳定性是嵌入之间的差与相邻矩阵之间的差之比。由于这种稳定性的定义取决于所涉及的矩阵的大小，本文定义了另一种称为相对稳定性的度量，它对相邻矩阵和嵌入矩阵的大小是不变的：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221122105135006.png" alt="image-20221122105135006" style="zoom:80%;" />

  进一步定义了稳定性常数：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221122105156180.png" alt="image-20221122105156180" style="zoom:80%;" />

  只要动态嵌入$F$具有小的稳定常数，就认为它是稳定的。显然，$K_S(F)$越小，嵌入$F$越稳定。在实验中，使用稳定性常数作为度量来比较DynGEM算法与其他方法的稳定性。

- 深度无监督学习相关的最新研究表明，自编码器能够成功的学习各类任务复杂的低维向量表示。DynGEM使用深度自编码器将输入数据映射到一个高度非线性的潜在空间，该方法能捕捉图快照在演化过程的连续趋势。该方法是半监督的方法，目标函数选择一阶近似和二阶近似的组合。

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221122105351617.png" alt="image-20221122105351617" style="zoom:80%;" />

  节点$v_i$的邻域由$s_i∈R^n$表示，对于图$G_t$中的任意一对节点$v_i$和$v_j$，模型将它们的邻域作为输入：$x_i＝s_i$和$x_j＝s_j$，并将它们传递给自动编码器，以在编码器的输出端生成多维嵌入$y_i＝y_i^{(K)}$和$y_j＝y_j^(K)$。解码器从嵌入$y_i$和$y_j$中重建邻域$\hat x_i$和$\hat x_j$。

-  处理规模不断增长的动态图需要一个良好的机制来扩展自动编码器模型，同时保留以前训练时间步骤的权重。一个关键部分是决定隐藏层的数量和隐藏单元的数量应该如何随着更多的节点被添加到图中而增长。本文提出了一个启发式方法，即PropSize，用于计算所有层的新的层大小，确保连续层的大小在一定的系数内。这个启发式方法计算每个时间步的神经网络层的新尺寸，并在需要时插入新层。对于编码器来说，从其输入层$(l_1=x)$和第一个隐藏层$(l_2=y^{(1)})$开始，为每一对连续的层$(l_k和l_{k+1})$计算层宽，直到每个连续的层对满足以下条件：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221122110451773.png" alt="image-20221122110451773" style="zoom:80%;" />

  其中$0 < ρ < 1$是一个合适的超参数。如果任何一对$(l_k,l_{k+1})$不满足该条件，$l_{k+1}$的层宽就会增加以满足启发式的要求。请注意，嵌入层$y =  y^{(K)}$的大小总是固定在$d$，从不扩大。如果编码器的倒数第二层和嵌入层不满足PropSize规则，就在中间增加更多的层（尺寸满足规则），直到规则得到满足。这个过程也适用于解码器层，从输出层$(\hat x)$开始，一直到嵌入层（或$\hat y=\hat y^{(K)}$）来计算新的层大小。在决定了层数和每层的隐藏单元数之后，采用Net2WiderNet和Net2DeeperNet方法来扩展深度自动编码器。Net2WiderNet允许扩大层数，即在现有的神经网络层中增加更多的隐藏单元，同时大致保留该层正在计算的函数。Net2DeeperNet通过使新的中间层紧密复制身份映射，在两个现有层之间插入一个新的层。这对ReLU激活可以做到，但对sigmoid激活不能做到。利用PropSize、Net2WiderNet和Net2DeeperNet在每个时间步长上扩大和加深自动编码器的组合，使得能够处理节点数量随时间增长的动态图，并且如本文的实验所显示的那样，提供了显著的性能改进。

### Dynamic Network Embedding by Modeling Triadic Closure Process

- 网络嵌入，旨在学习顶点的低维表示，是一项重要的任务，最近吸引了大量的研究工作。在现实世界中，网络，如社会网络和生物网络，是动态的，并随着时间的推移而不断发展。然而，几乎所有现有的网络嵌入方法都集中在静态网络上，而忽略了网络的动态。本文提出了一种新的表示学习方法DynamicTriad，以保留给定网络的结构信息和演化模式。方法的总体思路是强加三元组，三元组是由三个顶点组成的，是网络的基本单位之一。特别地，模拟了一个封闭的三合会，它由三个相互连接的顶点组成，是如何从一个开放的三合会发展而来的，这个三元组有两个顶点是相互不连接的。这种三联体的闭合过程是网络形成和演化的基本机制，从而使我们的模型能够捕捉到网络的动态，并在不同的时间步骤中学习每个顶点的表征向量。在三个真实世界的网络上的实验结果表明，与几个最先进的技术相比，DynamicTriad在几个应用场景中取得了实质性的进展。例如，该方法可以有效地应用并**帮助识别移动网络中的电话欺诈，以及预测贷款网络中用户是否会偿还贷款**。

  ![image-20221121193553004](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121193553004.png)

  上图是一个动态社会网络的图示，用户A和B有不同的社交策略。例如，用户A倾向于介绍她的朋友相互联系，而用户B则倾向于将她的朋友留在自己的community。

- 本文的目标是通过捕捉网络的结构演化特性来学习顶点的嵌入向量。也就是说，给定一个从时间步骤1到$T$的网络快照序列$G_1, ...,G_T$，目标是设计一个模型能在每个时间步$t$学习顶点$i$的嵌入向量$u_i^t$。

  然而，要保留动态网络的时间信息是非常具有挑战性的。现有的工作对潜空间如何随时间演变的建模发展有限。目前大多数模型使用的典型解决方案是，通过在目标函数中加入正则化项，假设顶点只会在潜空间中随时间平滑移动，然而这个假设并不总是成立，因为一些顶点的结构可能会急剧演变。此外，这种方法不能捕捉到顶点进化模式之间的差异。

  本文提出一种新的动态网络嵌入方法，即DynamicTriad，该模型的总体思路是强加三联体（即三个顶点为一组）来模拟网络结构的动态变化。总地来说，有两种类型的三元组：封闭式三元组和开放式三元组。在一个封闭的三元组中，对于任何两个顶点，它们之间存在着一种关系。在一个开放的三角形中，只有两个关系，这意味着三个顶点中的两个没有相互连接。模拟一个封闭的三联体如何从一个开放的三联体发展而来，这被称为三联体封闭过程，是动态网络形成和演化的一个基本机制。特别是，本文设计了一个统一的框架来量化一个开放三角形发展为封闭三角形的概率，并在不同的时间步骤中共同学习每个顶点的嵌入向量。

- DynamicTriad的目标是对三元组闭合过程进行建模，它描述了开放的三元组如何演变成封闭的三元组，并反映了不同顶点的特性之间的差异。以社会网络为例来介绍描述模型，它也可以应用于其他类型的网络。从一个在时间$t$上开放的三联体$(v_i,v_j,v_k)$的例子开始，其中用户$v_i$和$v_j$互不认识，但他们都是$v_k$的朋友（即$e_{ik},e_{jk}∈E_t$和$e_{ij}  /∈E_t$）。现在，用户$v_k$将决定是否介绍$v_i$和$v_j$，让他们互相认识，并在下一个时间步骤$t+1$时在他们之间建立联系。我们自然地假设$v_k$将根据她与$v_i$和$v_j$的亲密程度（在隐藏空间）做出决定，这由一个$d$长度的向量$x^t_{ijk}$量化为：

  ![image-20221121194947374](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121194947374.png)

  其中，$w^t_{ik}$表示时间$t$时$v_i$和$v_k$的联系强度，$u_i^t$是时间$t$时$v_i$的嵌入向量。此外，定义了一个社会策略参数$θ$，它是一个$d$维向量，用于提取嵌入在每个节点的潜在向量中的社会策略信息。基于上述定义，定义开放的三元组$(v_i,v_j,v_k)$演变为封闭的三人组的概率，即$v_i$和$v_j$在时间$t+1$时，在$v_k$的介绍（或影响）下，他们之间形成联系的概率为：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121195423684.png" alt="image-20221121195423684" style="zoom:80%;" />

  值得一提的是，$v_i$和$v_j$可能是由他们几个共同的朋友介绍的。因此，下一个目标是共同模拟具有一对共同的非链接顶点的多个开放三角形是如何演变的。为此，定义$B^t(i, j)$为$v_i$和$v_j$在时间步$t$的共同邻居，并定义一个向量![image-20221121195719751](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121195719751.png)，如果开放三角形$(v_i,v_j,v_k)$将在$t+1$发展成一个封闭三角形，则<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121195925687.png" alt="image-20221121195925687" style="zoom:67%;" />。换句话说，在$v_k$的影响下，$v_i$和$v_j$将成为朋友。一旦$(v_i,v_j,v_k)$成为封闭的，所有与$v_i$和$v_j$相关的开放三角关系都将成为封闭的。因此，通过进一步假设每个共同的朋友对vi和vj的潜在链接的影响之间的独立性，我们定义一个新的链接eij将在时间步骤t+1创建的概率为Pt tr+（i，j）=∑αt。 i,j =0 ∏ k∈B t (i,j ) (Pt tr(i, j, k))αt,i,j k× (1-P t  tr(i, j, k))(1-αt,i,j k) (3)  同时，如果vi和vj没有受到他们任何共同朋友的影响，边eij将不会被创建。我们把它的概率定义为Pt tr- (i, j)= ∏ k∈B t  (i,j )(1-P t tr(i, j, k)) (4) 为了把上述两个可能的开放三角形（vi,vj,vk）的进化轨迹放在一起，我们定义St  +={(i, j)|eij / ∈ Et ∧ eij ∈ Et+1}表示在时间步骤t+1时成功创建的链接，让集合St - = {(i,  j)|eij / ∈ Et ∧ eij / ∈  Et+1}表示那些没有被创建的。然后，我们将三元组闭合过程的损失函数定义为数据的负对数似然。Lttr = - ∑ (i,j)∈St + log P t tr+ (i, j) - ∑ (i,j)∈St - log P t tr- (i, j) (5)  社会同质性和时间平稳性。我们利用另外两个假设来加强DynamicTriad：社会同质性和时间平稳性。社会同质性表明，高度连接的顶点应该紧密地嵌入到潜在的表示空间中。形式上，我们将两个顶点vj和vk的嵌入utj和utk之间的距离定义为gt(j, k)=||utj - utk||22 (6) 在当前时间步骤t，我们将所有的顶点对分为两组，即边Et + = Et和非边Et -=  {ejk|j∈{1, -- ,N},k∈{1, -- ,N},j
  = k, ejk / ∈  Et}。根据同质性假设，如果两个顶点之间有联系，它们往往在潜在代表空间中被嵌入得更近，这导致我们对社会同质性的基于排名损失的函数为Ltsh=∑（j, k)∈Et + (j ′ ,k′ )∈E t - h(wjk, [gt(j, k) - gt(j′,k′)+ξ]+) (7) 其中[x]+  =max(0,x)为任意实数x，ξ∈R+为边际值。函数h(-, -)结合了权重和差异度量，通常定义为h(w, x)=w -  x。很自然地假设一个网络会随着时间平滑演化，而不是在每个时间步骤中完全重建。因此，我们通过最小化相邻时间步骤中嵌入向量之间的欧氏距离来定义时间平滑性。形式上，相应的损失函数是 Ltsmooth = {∑N i=1 ||uit - ut-1 i ||22 t>。

### Embedding Temporal Network via Neighborhood Formation

KDD2018

- 鉴于网络挖掘在现实生活中的丰富应用，以及近年来表示学习的蓬勃发展，网络嵌入已经成为学术界和工业界越来越多研究兴趣的焦点。然而，在现有的研究中，以节点间连续互动事件为特征的网络的完整时间形成过程还很少被建模，这就需要对所谓的时间网络嵌入问题进行进一步研究。有鉴于此，在本文中引入了邻域形成序列的概念来描述节点的演化过程，在序列中的邻域之间存在时间激发效应，因此我们提出了基于Hawkes过程的时态网络嵌入（HTNE）方法。HTNE很好地将Hawkes过程整合到网络嵌入中，从而捕捉到历史邻居对当前邻居的影响。特别是，低维向量的相互作用分别作为基本概率和时间影响被送入霍克斯过程。此外，注意力机制也被整合到HTNE中，以更好地确定历史邻居对节点当前邻居的影响。在三个大规模的真实网络上的实验表明，从提出的HTNE模型中学习到的嵌入在各种任务（包括节点分类、链接预测和嵌入可视化）中取得了比最先进方法更好的性能。特别是，基于节点嵌入推断的到达率的时间推荐显示了所提模型的出色预测能力。

- 最近关于网络嵌入方法的工作通常通过考虑各种上下文信息，如节点的邻居，来关注静态网络结构。这些方法的一个经常被忽视的假设是，一个节点的邻居是无序的，即连接形成的历史过程被省略了。然而在现实中，网络是通过依次增加节点和边来形成的，这确实应该被视为一个由节点和其邻居之间的互动事件驱动的动态过程。因此，一个节点的邻域并不是同时形成的，观察到的网络结构snapshot是某些时间段内邻域的积累。

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121175956439.png" alt="image-20221121175956439" style="zoom:80%;" />

  例如，上图(a)显示了一位作者的网络：节点1及其邻居，即节点2至6。从网络结构的snapshot角度来看，我们只观察到最新的合著关系，而这些节点是如何以及何时连接起来的仍然是未知数。事实上，在大多数真实的网络中，节点之间的边一般是由连续的事件建立的，这就构成了所谓的时序性网络。例如，合著者网络是由具有明确时间戳的合著论文驱动的，每条边都按时间顺序标注了节点1和其邻居之间共同撰写的几篇论文。因此，自我的时间网络可以根据事件的时间展开成一个特定于节点的邻居序列，这被定义为邻居形成序列，如上图(b)所示。

- **Temporal Network**：时序网络的边由节点之间的时间性互动事件诠释，可以表示为$G =< V, E; A  >$，其中$V$表示节点集，$E$表示边集，$A$表示事件集。节点$x$和$y$之间的每条边$(x, y)∈E$视为按时间顺序发生的事件，即$a_{x,y} = {a1 → a2 → ...}  ⊂A$，其中$a_i$表示时间戳为$t_i$的事件。考虑到时间网络可以更明确地描述共同作者的演变，这也可以为预测一个节点的未来共同作者提供线索。因此，网络中一个节点的相邻的邻居可以根据与邻居互动事件的时间递增来组织成一个序列，代表邻居的形成过程。

- **Neighborhood Formation Sequence**：给定时序网络$x∈V$中的一个源节点，该节点的邻居关系是$N (x ) = \{y_i |i = 1, 2, ...\}$，该节点与每个邻居之间的边被注释为时间上的互动事件$a_{x,y_i}$。在数学上，邻域形成序列可以表示为一系列的目标邻居到达事件，即${x : (y_1,  t_1) → (y_2, t_2) →...→ (y_n, t_n )}$，每个元组代表节点$y_i$在时间$t_i$形成为节点$x$的邻居的事件。值得一提的是，一个节点的邻居通常表示一个不重复的节点集。而根据邻居形成序列的定义，每个邻居可以在序列中重复出现，以代表与源节点的多次交互。有了定义的序列，节点连接随时间的变化就可以明确地表现出来，这样就可以从序列中推断出网络中节点的隐藏结构。再以合著者网络为例，一个节点的邻域形成序列显示了其合著者的变化，可以帮助推断出作者的研究兴趣。此外，邻居关系形成序列中的事件并不是独立的，因为历史上的邻里关系形成事件会影响当前的邻里关系形成。例如，一个研究者可能专注于一个特定的领域，如数据挖掘，因此，共同作者主要是数据挖掘研究者。但当近年来深度学习成为研究的焦点时，我们可能会在他/她的合著者序列中逐渐观察到一些人工智能研究者，并可以从最近的邻居序列中预测，作者可能会将研究兴趣转向人工智能，未来的合著者可能会有更多的人工智能人士。因此，本文旨在考虑动态邻居形成序列，以学习节点的表征。

- **Hawkes Process**：

  点过程通过假设时间$t$之前的历史事件可以影响当前事件的发生而对离散的连续事件进行建模，条件强度函数表征顺序事件的到达率，可以定义为在给定所有历史事件的情况下，在一个小的时间窗口$[t, t + ∆t )$中发生的事件数量$H(t)$：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121181819884.png" alt="image-20221121181819884" style="zoom:80%;" />

  霍克斯过程是一个典型的时间点过程，条件强度函数定义如下：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121182030139.png" alt="image-20221121182030139" style="zoom:80%;" />

  其中$μ(t)$是某一事件的基础强度，显示时间$t$的自发事件到达率。$κ(.)$是一个核函数，用来模拟过去历史对当前事件的时间衰减效应，通常是指数函数的形式。

  霍克斯过程的条件强度函数表明，**当前事件的发生不仅取决于最后一个时间步的事件，而且还受到具有时间衰减效应的历史事件的影响。这种特性对于邻域形成序列的建模是可取的，因为当前的邻域形成可以受到较近期事件的影响，而发生在较长历史时期的事件对当前目标邻域的发生贡献较小**。为了处理不同类型的到达事件，Hawkes过程可以扩展到多变量的情况，其中条件强度函数被设计为每个事件类型的一个维度。

- **用多变量霍克斯过程建立邻域形成模型**：

  当把每个节点看作是一个源时，就会产生一个由互动事件驱动的目标邻居序列。在x的序列中，目标y的到达事件的条件强度函数可以表述为：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121182902622.png" alt="image-20221121182902622" style="zoom:80%;" />

   其中$μ_{x,y}$代表$x$和$y$之间形成边的事件的基本概率，而$h$是时间$t$之前节点$x$的邻域形成序列中的历史目标节点。$α_{h,y}$代表历史邻居$h$对当前邻居$y$的激发程度，内核函数$κ(.)$表示时间衰减效应，可以写成指数函数的形式：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121183025672.png" alt="image-20221121183025672" style="zoom:80%;" />

   衰减率$δ$是一个与源相关的参数，这说明对于每个源节点，历史上的邻居可以以不同的强度影响当前的邻居形成。按照公式中的强度函数，可以对每个源节点的邻居形成顺序进行建模。接下来，为了学习网络中节点的$D$维表征，假设每个节点由一个$D$维向量表示，并输入强度函数。具体来说，假设节点i的节点嵌入是$e_i$，那么连接源$x$和目标$y$的基本概率可以从函数<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121183210385.png" alt="image-20221121183210385" style="zoom:67%;" />中映射出来。直观地说，基数揭示了源节点$x$与目标节点$y$的自然亲和力。因此，为了简洁起见，使用负的平方欧氏距离作为相似性度量来捕捉节点x和y的嵌入之间的亲和力，即$μ_{x,y} = f (e_x , e_y ) = -||e_x - e_y ||^2$。同样，在计算对当前节点的历史影响$α_{h,y}$时，我们使用同样的相似性度量$α_{h,y} = f (e_h, e_y ) = -||e_h - e_y ||^2$。由于我们引入的相似性度量值为负值，我们应用指数函数将条件强度率转为正实数，即<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121183423207.png" alt="image-20221121183423207" style="zoom:80%;" />，因为$λ_{y |x} (t )$在被视为单位时间的速率时应取正值。然后，可以将邻居的条件强度函数定义为：

  ![image-20221121183508568](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221121183508568.png)

  

### Continuous-Time Link Prediction via Temporal Dependent Graph Neural Network

- 最近，图神经网络（GNNs）被证明是学习网络节点表征的有效工具，并在半监督的节点分类任务中取得了良好的表现。然而，大多数现有的GNNs方法未能考虑到网络的时间信息，因此，不能很好地应用于动态网络应用，如连续时间链接预测任务。为了解决这个问题，本文提出了时间依赖图神经网络（TDGNN），一个简单而有效的动态网络表示学习框架，它将网络的时间信息纳入GNNs。TDGNN引入了一个新颖的时间聚合器（TDAgg）来聚合邻居节点的特征和边缘的时间信息以获得目标节点的表示。具体来说，它使用指数分布来分配邻居节点的聚合权重，以偏重不同边的时间信息。该方法的性能已经在六个真实世界的动态网络数据集上得到验证，用于连续时间链接预测任务。实验结果表明，所提出的方法优于几个最先进的baseline。

- TDGNN的第一步是获得两个目标节点的表示向量。与目前的GNN方法不同的是，GNN方法是通过聚合其具有相同权重的邻居节点特征来获得目标节点的表示；所提出的方法引入了时间聚合器（TDAgg），它使用指数分布来分配邻居节点特征的聚合权重，以偏重不同边缘的时间信息，也就是说，与目标节点连接时间t更近的邻居节点具有更大的聚合权重。第二步是使用Edge  Aggregator（EdgeAgg）获得边缘表示，它将两个目标节点的表示向量映射为边缘表示向量，在这项工作中，我们介绍并比较了六个不同的EdgeAggs。

- **GNNs**

  GCN是频谱图卷积方法的近似，它高度依赖于图的拉普拉斯算子：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221119161144264.png" alt="image-20221119161144264" style="zoom:80%;" />

  其中，L是实对称正定半无限矩阵，I是单位矩阵，A是相邻矩阵，D是度矩阵。对于一个给定的静态无定向网络G = (V , E)，在第l层的节点vi∈V表示可以通过GCN得到如下结果：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221119161259331.png" alt="image-20221119161259331" style="zoom:80%;" />

  其中σ是一个非线性激活函数，$W_l$是GCN在第$l$层的参数，$N(u)$是目标节点$v_i$的邻居节点集。从该公式可以看到GCN对邻居节点表征<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221119161427334.png" alt="image-20221119161427334" style="zoom:67%;" />分配相同的权重，因此，它忽略了网络中的边缘信息，特别是对于包含时间信息的动态网络。

- **TEMPORAL DEPENDENT GRAPH NEURAL NETWORK (TDGNN)**

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221119161510363.png" alt="image-20221119161510363" style="zoom:67%;" />

  上图是TDGNN的概述，TDGNN的输入是一个动态图$G = (V , E^T ,T , X )$，其中<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221119162247656.png" alt="image-20221119162247656" style="zoom: 80%;" />是节点特征，n是节点数，P是节点特征的维度。节点对$(v_A, v_B)$可以由多条边组成，这些边在不同的时间t - 1和t - 2产生。TDGNN的输出是新的节点表征<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221119162401311.png" alt="image-20221119162401311" style="zoom:80%;" />和边的表征<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221119162557462.png" alt="image-20221119162557462" style="zoom:80%;" />在时间t，其中d和m分别是节点和边表征的维度。

  首先，为了介绍TDGNN如何学习网络结构和时间信息，为简单起见，在单层前向TDGNN的背景下进行描述，它可以很容易地被推广到多层网络。假设节点v是目标节点，初始节点表示<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221119162703749.png" alt="image-20221119162703749" style="zoom:80%;" />。在时间t的新节点表示可以由TDAgg计算如下：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221119162733652.png" alt="image-20221119162733652" style="zoom:80%;" />

  其中，k代表TDGNN的第k层，σ是ReLU等非线性激活函数，$N_t(v)$代表目标节点v在时间t的邻居节点集，W是可学习的共享权重矩阵，时间t的邻居节点聚合权重$α^t_{vu}$可以计算如下：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221119162904613.png" alt="image-20221119162904613" style="zoom:80%;" />

  其中$t_{v,u}∈T$代表节点v和u之间的边生成时间。

  然后，对于要在时间t预测的节点对（v, u）之间的目标边$e^t_{v,u}$，可以通过边聚合函数EdAgg：<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221119163137995.png" alt="image-20221119163137995" style="zoom:80%;" />获得边表示<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221119163115981.png" alt="image-20221119163115981" style="zoom:80%;" />。选择并比较了六个常见的边缘聚合函数：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221119163158849.png" alt="image-20221119163158849" style="zoom:80%;" />

### *DySAT: Deep Neural Representation Learning on Dynamic Graphs via Self-Attention Networks

WSDM2020 DTDG

- 学习图中的节点表示对于许多应用（如链接预测、节点分类和社区检测）都很重要。现有的图表示学习方法主要针对静态图，而许多真实世界的图随着时间的推移而发展。复杂的时变图结构使得学习随时间变化的信息节点表示具有挑战性。本文提出了动态自注意网络（DySAT），用于学习节点表示以捕获动态图结构演化。具体来说，DySAT通过沿着structural neighborhood（结构邻域）和temporal dynamics（时序动态性）的两个维度的联合自注意力来生成节点表示。与最先进的图形演化建模的递归方法相比，动态自注意力是有效的，同时实现了一致的优异性能。在两种图类型上进行链路预测实验：通信网络和二分评级网络，实验结果表明，在单步和多步链路预测任务中，DySAT在几个最先进的图嵌入baseline上都有显著的性能提高。此外，消融研究验证了联合建模结构和时间自我关注的有效性。
- 学习图中节点的潜在表示（或嵌入），在社交媒体、生物信息学和知识图谱等不同领域中普遍存在。目标是学习低维向量，用以捕捉节点及其邻域的结构属性由于复杂的时变图结构，学习动态节点表示具有挑战性：节点可以出现和消失，链接可以出现和消失，社区可以合并和分裂。这要求学习的节点表示不仅保持结构接近性，而且还要联合捕获它们的时间演化规律。此外，多个潜在因素影响着图的演变，例如，在合作者网络中，不同研究领域的作者或处于不同职业阶段的作者可能会以不同的速度扩大他们的合作圈（潜在方面，如研究领域或职业阶段，作为建模时序图演化的独特视角）。因此，建模动态图表示学习中的多方面变化对于准确预测节点属性和未来链接至关重要。现有的动态图表示学习方法主要分为以下几类：时间正则化（temporal  regularizers），增强来自相邻快照的节点表示的平滑性，递归神经网络通过隐藏状态总结历史快照。平滑方法虽然在高稀疏性网络中有效，但当节点表现出明显不同的进化行为时，可能会失败。RNN虽然具有表现力，但需要大量的训练数据才能胜过静态方法，并且随着时间步数的增加，其扩展性很差。
- 注意力机制最近在许多顺序学习任务中取得了巨大成功，基本原理是学习一个函数，该函数聚合可变大小的输入，同时关注与给定上下文相关的部分。当一个序列同时用作输入和上下文时，它被称为自注意力机制。尽管注意力机制最初是为了促进递归神经网络（RNN）捕获长程依赖性而设计的，但纯自注意力网络被证明在实现机器翻译的最先进性能方面的有效性，同时显著提高了效率。由于动态图通常具有周期性模式，例如循环链接或社区，因此自注意力可以从所有过去的图快照中提取上下文，以自适应地为先前的时间步骤分配可解释的权重。本文提出了一种新的神经网络结构，称为动态自注意网络（DySAT），以学习动态图上的潜在节点表示。DySAT通过沿着两个维度（结构邻域和时序动态性）联合自注意力来生成动态节点表示：结构注意力通过自我注意力聚合从每个快照中的局部节点邻域中提取特征，而时间注意力通过灵活加权历史的表示来捕捉多个时间步中的图演化信息。为了对图形结构中的多方面变化进行建模，DySAT学习了结构和时序注意层中的多个注意头，这些注意头能够在不同的潜在子空间上实现联合注意。本文通过消融研究证明了联合结构和时间注意力的益处，并可视化了时间注意力权重，以说明DySAT在适应具有不同进化趋势的数据集方面的能力。本文的主要贡献如下：
  - 提出了一种名为动态自我注意网络的新型神经网络架构，该架构利用联合结构和时间自注意力进行动态图表示学习。
  - 提出了一种DySAT的模块化架构设计，具有堆叠的结构和时间自注意力层，与基于RNN的解决方案相比，该设计能够实现高效的计算。
  - 本文进行了广泛的实验，证明在单步和多步链路预测任务中，DySAT的性能始终优于最先进的方法。

- DySAT从上到下有三个模块：structural attention block（结构注意力模块）；temporal attention block（时序注意力模块）；以及graph context prediction（图的上下文预测）。本文的关键创新点在于将图的演化解耦为两个模块，从而实现动态节点表示的高效计算。结构和时序自注意力层一起建模图的演化，并可以通过层堆叠实现任意复杂度的图神经网络。

  ![image-20221128141459326](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221128141459326.png)

- **Structural Self-Attention**

  该层的输入是图的快照$G$和一组输入节点表示$\{x_v∈R^D ,∀v∈ V\}$ 其中$D$是输入的嵌入维度。初始层的输入被设置为每个节点的一个one-hot编码向量。输出是一组新的节点表示$\{z_v∈R^D ,∀v∈ V\}$ 。特别地，通过计算作为其输入节点嵌入的函数的注意力权重，结构自注意力层关注节点$v$（在快照$G$中）的邻居。结构注意力层的操作定义为：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221128143128694.png" alt="image-20221128143128694" style="zoom:80%;" />

  其中$Nv=\{u∈ V:(u，V)∈ E\}$是快照$G$中节点$v$的邻居集合；$W^s∈ R^{F×D}$是应用于图中每个节点的共享权重变换；$a∈  R^{2D}$是将作为前馈层实现的注意力函数参数化的权重向量；$||$表示concatenation操作，$σ(·)$是一个非线性激活函数。$A_{uv}$是当前快照$G$中链路$(u，v)$的权重。通过对$V$中每个节点的邻居的softmax操作获得的一组学习到的系数$α_{uv}$表示当前快照中节点$u$对节点$v$的贡献。使用LeakyRELU非线性来计算注意力权重，然后是输出表示的指数线性单元（ELU）激活。使用稀疏矩阵有效地在邻居上实现masked的自注意力，因为对于G中的所有非链接，$α_{uv}$为零。因此，应用于快照G的结构关注层通过相邻节点嵌入的自我关注聚合输出节点嵌入，这可以被视为在相邻邻居之间传递的单个消息。

- **Temporal Self-Attention**

  为了进一步捕获动态图中的时间演化模式，DySAT设计了一个时序自注意力层。该层的输入是特定节点$v$在不同时间步的表示序列。假设输入表示在每个时间步充分地捕获局部结构信息，这使得能够实现结构建模和时间建模的模块化分离。具体来说，对于每个节点$v$，将输入定义为$\{x^1_v，x^2_v，…，x^T_v\}，x^t_v∈R^{D'}$，其中$T$是时间步的总数，$D'$是输入表示的维数。该层的输出是每个时间步的$v$的新表示序列，即$\{z^1_v，z^2_v，…，z^T_v\}，z^t_v∈R^{F'}$维度是$F'$。考虑时间将节点$v$的输入和输出表示为<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221128150630993.png" alt="image-20221128150630993" style="zoom:67%;" />和<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221128150639335.png" alt="image-20221128150639335" style="zoom:67%;" />。

  时间自注意层的关键目标是捕捉图结构在多个时间步长的时间变化。节点$v$在时间步$t$的输入表示$x^t_v$编码了$v$周围的当前局部结构。使用$x^t_v$作为query来关注其历史表示$(＜t)$，跟踪$v$周围的局部邻域的演变，时间注意力完全取决于每个节点的时间历史，因此有助于跨节点的高效并行。将每个节点的局部邻域和时间历史解耦为独立层，是提高模型效率的关键因素之一。为了计算$t$时刻节点$v$的输出表示，使用注意力的缩放点积形式[33]，其中查询、键和值被设置为输入节点表示。首先通过线性投影矩阵将query、key和value转换到不同的空间<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221128152018280.png" alt="image-20221128152018280" style="zoom:67%;" />，<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221128152038633.png" alt="image-20221128152038633" style="zoom:67%;" />和<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221128152102879.png" alt="image-20221128152102879" style="zoom:67%;" />。允许每个时间步$t$在$t$之前（包括$t$）的所有时间步中参与，以保持自回归特性。时间自注意函数定义为：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221128152207945.png" alt="image-20221128152207945" style="zoom:67%;" />

  其中<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221128152222100.png" alt="image-20221128152222100" style="zoom:67%;" />是通过多头注意力函数和每个entry<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221128152719873.png" alt="image-20221128152719873" style="zoom:67%;" />的掩码矩阵<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221128152253205.png" alt="image-20221128152253205" style="zoom:67%;" />获得的注意力权重矩阵，用来增强自回归属性。为了编码时间顺序，将$M$定义为：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221128152925100.png" alt="image-20221128152925100" style="zoom:80%;" />

  当$M_{ij}=-∞$时，softmax导致零注意力权重，即$β^{ij}_v=0$，这将注意力从时间步$i$切换到$j$。

- **Multi-Faceted Graph Evolution**

  通过堆叠结构和时间自注意力层，本文的方法可以充分捕捉图演化的单一类型或方面。然而，真实世界的动态图通常沿着多个潜在方面发展，例如，不同类型（如科幻、喜剧等）的用户观影偏好的发展呈现出明显不同的时间趋势。因此，本文赋予了模型expressivity，以通过多头注意力从不同的潜在角度捕捉动态图演化。多头注意力创建了多个独立的注意力功能实例，称为注意力头部，在输入嵌入的不同部分上操作，被广泛用于改善注意力机制的多样性。这有助于通过沿着多个潜在面的投影对不同子空间进行联合注意力。多头注意力还通过避免预测偏差来提高模型容量和稳定性。在结构层和时间层中使用多个注意头：

  - Structural multi-head self-attention（结构多头自注意力）：在每个结构注意层中计算多个注意头部，然后连接以计算输出表示。

    <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221128160043927.png" alt="image-20221128160043927" style="zoom: 80%;" />

    其中$H_S$是注意头的数量，$h_v∈ R^{F}$是多头注意力后节点$v$的输出。虽然结构注意力在每个快照中独立应用，但结构注意力头部的参数在不同快照中共享。

  - Temporal multi-head self-attention（时间多头自注意力）：与上述设置类似，在历史时间步上计算多个时间注意力头部，以计算最终节点表示。

    <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221128160738864.png" alt="image-20221128160738864" style="zoom:80%;" />

    其中$H_T$是时间注意力头部的数量，$H_v∈ R^{T×F'}$是时间多头注意力的输出。

- **DySAT Architecture**

  用于动态图表示学习的神经网络架构DySAT使用定义的结构和时间自注意力层作为基本模块。输入是T个图快照的集合，输出是每个时间步的节点表示。DySAT由一个结构模块和一个时间模块组成，其中每个模块包含对应层类型的多个堆叠层。结构模块通过自注意力聚合和叠加从每个节点的高阶局部邻域中提取特征，以计算每个快照的中间节点表示。然后，该节点表示序列输入到时间模块，该时间模块在多个历史时间步上进行处理，捕捉形结构中的时序性变化。时间模块的输出包括最终动态节点表示的集合，其被优化以在每个时间步中保持局部图上下文。

  - Structural attention block（结构注意力模块）。该模块由多个堆叠的结构自关注层组成，以从不同距离的节点提取特征。使用共享参数在每个图快照上应用每个层，以在每个时间步捕获节点周围的局部结构，输入层的嵌入可能会在不同的快照中发生变化。将结构注意力模块输出的节点表示表示为$\{h^1_v，h^2_v，…，h^T_v\}，h^t_v∈R^F$，其作为输入输送到时间注意力模块。

  - Temporal attention block（时间注意力模块）。首先通过使用position embedding$\{p^1，…，p^T\}，p^t∈R^F$来捕获时间注意力模块中的时序信息，其嵌入每个快照的绝对时间position。然后将position embedding与结构注意力模块的输出相结合，以获得多个时间步长上节点$v$的的输入表示序列<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221128164113292.png" alt="image-20221128164113292" style="zoom:67%;" />。该模块也遵循具有多个堆叠的时间自注意力层的类似结构。最终层输出传递到位置前馈层，以给出最终节点表示<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221128164148434.png" alt="image-20221128164148434" style="zoom:67%;" />。

  - Graph context prediction（图的上下文预测）。为了使学习到的表示能够捕捉结构演化，目标函数在多个时间步中保持节点周围的局部结构。在时间步$t$，$e^t_v$使用节点$v$的动态表示来保持$t$处$v$附近的局部接近性。特别地，在每个时间步使用二进制交叉熵损失来鼓励在固定长度随机游走中共同出现的节点具有类似的表示。

    <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221128165324209.png" alt="image-20221128165324209" style="zoom:67%;" />

    其中$σ$是sigmoid函数，$<.>$表示内积运算，$N^t_{walk}(v)$是在$t$时刻快照的固定长度随机游走上与$v$共同出现的节点集合，$P^t_n$是快照$G_t$的负采样分布（通常定义为度分布的函数），$w_n$是负采样率，是平衡正和负采样的可调超参数。

### *Learning Dynamic Preference Structure Embedding From Temporal Networks

ICBK2021

- 时序网络的动态性在于节点之间的持续互动，随着时间的推移，节点的偏好也呈现出动态的特点。因此，挖掘时态网络的挑战是双重的：网络的动态结构和动态节点偏好。本文研究了动态图抽样问题，旨在与GNN合作，动态地捕捉节点的偏好结构。提出的动态偏好结构（DPS）框架包括两个阶段：结构采样和图融合。在第一阶段，设计了两个参数化的采样器，以适应性地学习偏好结构与网络重建任务。在第二阶段，设计了一个额外的注意层来融合一个节点的两个采样的时间子图，为下游的任务生成时间节点嵌入。在许多现实生活中的时态网络上的实验结果表明，由于学习了自适应的偏好结构，DPS大大超过了几个最先进的方法。

- 以前的方法主要借助于递归机制来捕捉动态的节点偏好，例如递归神经网络和时间点过程。递归机制使用节点的历史互动来预测未来的行为，这比网络建模更注重顺序建模。最近，图神经网络（GNNs）在图学习任务中显示出巨大的成功。与GNNs相比，递归机制失去了网络中的高阶协作过滤信号。

  ![image-20221114110443504](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221114110443504.png)

  例如，递归方法不善于推断上图(a)中节点U和节点V之间的二阶接近性。虽然一些混合方法弥补了序列建模和网络挖掘之间的差距，但这些方法依赖于静态网络的序列，不能自适应地处理时态网络的动态结构。然而，最近提出的基于启发式抽样策略的时态GNN,很难捕捉到动态节点的偏好。这些启发式策略统一或按时间顺序对节点的邻域进行采样，没有考虑到个性化的偏好结构，这在长程偏好建模中可能会失败。

- 为了克服上述两个挑战，我们研究了从时态网络的动态结构中自适应取样的问题，使GNN意识到时态模式。我们的动机有三个方面。首先，用少数采样的邻居生成高质量的节点嵌入是合理的。第二，时态网络的偏好结构会随着时间的推移而向前演化，正如递归方法]中所显示的。第三，抽样方法可以显著提高GNN的训练效率。我们提出的动态偏好结构（DPS）框架包括两个阶段：结构采样和图融合。在第一阶段，我们设计了两个参数化的采样器来自适应地捕捉偏好结构：**时间衰减采样（TDS）**被提出来用参数化的时间衰减分布来捕捉每个节点的时间模式；而**Gumbel注意力采样（GAS）**被提出来用浅层图神经网络来编码节点的语义接近性。在第二阶段，我们设计了一个基于注意力的融合层来融合每个节点的不同采样子图的节点嵌入。这些采样子图是由预先训练好的TAS和GAS采样器分别从时态网络的动态结构中产生。我们的贡献可以概括为以下几点：我们设计了两个参数化采样器TDS和GAS，以选择具有网络重建任务的时态网络的偏好结构。我们进一步设计了一个**基于注意力的融合层**，以融合由TDS和GAS采样的不同时空子图的节点嵌入。我们在广泛的时态网络上进行了实验，结果证明了我们提出的DPS框架的效率。

- <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221114111039473.png" alt="image-20221114111039473" style="zoom:67%;" />

  上图是2层动态偏好结构（DPS）框架的架构。在t3之前给定一个节点U，我们递归地提取其2跳自我网络。在结构取样阶段，取样器，包括时间衰减取样（TDS）和Gumbel注意力取样（GAS），对每个节点的邻域进行估值，并根据其重要性分布取样两个邻域。采样过程递归地重复两次。在图融合阶段，一个基于注意力的融合层结合了来自不同子图的节点嵌入，每个子图都产生一个指定的节点嵌入。

  图结构采样阶段的目的是捕捉时序网络随时间变化的偏好结构。一方面，所提出的时间衰减采样（TDS）的动机是随着时间的流逝，节点间交互的影响衰减。另一方面，提议的Gumbel注意力采样（GAS）是由图神经网络（GNN）的节点的语义接近性所启发的。

- **Time Decay Sampling:**

  时间衰减采样TDS通过为每个节点$u$学习一个个性化的时间衰减分布来评估历史互动的邻域意义。令$N (u, t) = {(u, v, t_k)|(u, v, t_k) ∈ E \ and \ t_k <  t}$是节点$u$在$t$之前的邻域交互，$t_k$是一个交互的时间戳。节点的历史互动对其未来行为的影响随着时间的推移而减少。TDS函数根据$N(u,t)$保留的影响程度从$N(u,t)$中抽样出一些交互事件。$N(u,t)$的归一化影响在这里是一个分类分布，遵循指数衰减率，写为：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221124151800064.png" alt="image-20221124151800064" style="zoom:80%;" />

  这只与时间上的接近性有关，一旦得到$λ_u$和相应的交互集合$N(u,t)$，就从$N(u,t)$中抽样出一些交互，用

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221124151952814.png" alt="image-20221124151952814" style="zoom:80%;" />

  表示，其中$s$是样本大小。

  TDS优化：为了避免对$λ_u$进行繁琐的超参数搜索，建议使用时序链接重复来估计一个合适的衰减率$λ_u$。任务是找到在$N(u,t)$中最近的时间$t_v$重复的交互$(u, v,  t)$。重复的概率用$p_{(u,t)}(t_v)$表示。对于一个指定的节点$u$的目标是通过迭代$u$的交互事件使时序链接重复的可能性最大化，写作：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221124152331841.png" alt="image-20221124152331841" style="zoom:80%;" />

  其中为了数值稳定，衰减率被约束为$λ_u ≤  100$，这是一个常见的的对数和凸问题，可以用凸编程求解器来解决。这个任务可以很好地捕捉到节点$u$与$λ_u$的重复时间模式，表明个性化的节点偏好。此外，该公式中忽略了过去没有发生的交互，对这些交互的分析需要高阶方法，如GNNs。

- **Gumbel Attention Sampling：**

  TDS用来捕捉时间模式的个性化节点偏好，但由于缺乏潜在的嵌入方法，很难理解节点的语义接近性。将Gumbel-softmax技巧和GNN结合起来，GNN在图学习中取得了显著的成功，可以捕捉时序网络的语义偏好结构。与TDS一样，Gumbel注意力抽样（GAS）的目标是在给定一个节点$u$和一个时间点$t$时，从$N(u,t)$中抽样出一些交互事件。  

  Gumbel-softmax技巧：令$h^t_u$是GNN在输入层产生的$(u,t)$的时间节点嵌入。然后，$(v,t_k)$相对于$(u,t)$通过交互事件$(u, v,  t_k)$的非归一化注意力分数被表述为

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221124154440509.png" alt="image-20221124154440509" style="zoom:80%;" />

  其中$W^Q$是query的transform矩阵，$W^K$是key的transform矩阵，$·$是向量间的内积。尽管注意力分数可以代表节点之间的语义接近性，但注意力机制并不适合对偏好结构进行采样，以提供更好的时间节点嵌入。Gumbel-softmax技巧被进一步引入，以实现从注意力分数中采样的训练过程。$N(u,t)$中$(u, v,t_k)$的Gumbel-softmax概率计算为:

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221124155402422.png" alt="image-20221124155402422" style="zoom:80%;" />

  其中$g$来自Gumbel分布，$τ$是softmax的temperature。在训练开始时，一个大的$τ$会使注意力分数更加平稳。在训练过程中，当温$τ→0$时，注意力分数接近真实分布$p_{(u,t)}$。 

  GAS优化：时序链接预测，预测节点未来链接的概率，被用来训练GAS的单层GNN。在每个前向传递中，GAS根据

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221124155553397.png" alt="image-20221124155553397" style="zoom:80%;" />

  对节点$u$的交互进行采样。$u$在第一层的时序节点嵌入计算为

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221124155620701.png" alt="image-20221124155620701" style="zoom:80%;" />

  其中$W^V$是value的transform矩阵。新的节点嵌入是选定的相交互事件的加权和，时序链接预测被用作训练任务，因为它详细地描述了节点的动态性，就像TDS优化公式中的时间链接重复。由于时序网络的动态结构，GAS只使用单层网络进行图卷积。

- 基于采样的图形卷积：本文提出的TDS和GAS采样器分别根据公式进行预训练。令$S^{TDS}(u, t)$和$S^{GAS}(u,  t)$为节点$u$在时间$t$的采样邻居集合。在图卷积过程中，这些预训练的采样器取代启发式的统一采样器来检索指定节点的偏好结构。邻居集合中的一个交互$e = (u, v,t_k)$携带多种信息：邻居身份$v$，时间戳$t_k$，以及互动特征$m^{t_k} _{uv}$。然后，这些信息被一些编码函数转化为密集的向量，用于神经网络的反向传播。设$h^{l-1}_v$是$v$在$(l-1)$层的节点嵌入，它是0层的one-hot编码。时间核函数用余弦函数编码时间戳，定义为

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221124165245679.png" alt="image-20221124165245679" style="zoom:80%;" />

  其中$Δt = t -  t_k$是两个时间戳之间的时间间隔，$ω_1$是余弦函数频率的可训练参数，$d$是输出向量的维度。这些密集的向量被连接起来，以表示交互嵌入，即：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221124165338434.png" alt="image-20221124165338434" style="zoom:80%;" />

  对于每个邻居集合，采用基于注意力的图卷积层，写成：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221124165415335.png" alt="image-20221124165415335" style="zoom:80%;" />

  其中$h^{l-1}_u (t)$是$u$在$(l - 1)$层的嵌入，$W^Q$,$ W^K$ , $W^V$是转换矩阵，$α_{ue}$是节点$u$的交互事件$e = (u, v,t_k)$的注意分数，为清晰起见，省略了节点的时间戳。

  采样子图融合：设$h^{TDS}_u(t)$和$h^{GAS}_u(t)$为不同结构采样器对$u$的节点嵌入，为清晰起见，省略了图卷积层。由于TDS和GAS是为不同的目的设计的，它们的输出节点嵌入代表了节点的特定结构偏好。因此，建议增加一个融合层，即：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221124165709729.png" alt="image-20221124165709729" style="zoom:80%;" />

  其中$q$是一个共享的注意力向量，$W^S$是每个采样器的特定变换矩阵，$b$是偏置项。不同取样器的注意力分数通过softmax函数归一化，写成：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221124165803633.png" alt="image-20221124165803633" style="zoom:80%;" />

  u的节点嵌入最终被融合为：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221124165847655.png" alt="image-20221124165847655" style="zoom:80%;" />

### Instant Graph Neural Networks for Dynamic Graphs

2022

- 基于快照对图的变化进行建模的方法存在两个缺陷：首先，图的变化反映在图表示，从而导致模型的准确性损失有一个实质性的延迟;第二，反复在每个快照计算整个图像的表示矩阵是耗时的，严重限制了可伸缩性。

- 这篇论文提出了InstantGNN，它是一种对动态图表示矩阵的增量计算方法，不需要耗时的重复计算，从而允许实时的图表示更新和预测。此外，该方法提供了一个自适应训练策略，指导模型再训练的时候,能获得最大的性能收益。

- 基于广义增量框架来增量更新表示矩阵，当图发生变化时在本地调整受到影响的节点的表示向量，下一步通过传播操作计算变化对其余节点的影响。

- 定义了PPR得分反映节点t∈V对节点s的相对重要性：![image-20221031215339605](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221031215339605.png)

  $\alpha$是传输概率，$e_s$是one-hot向量。

  同时,维持一个向量r指示每个元素，节点已经收到了但是没有传播的累积信息。

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221031215600356.png" alt="image-20221031215600356" style="zoom:50%;" />

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221031215633160.png" alt="image-20221031215633160" style="zoom:50%;" />

### EvolveGCN: Evolving Graph Convolutional Networks for Dynamic Graphs

AAAI20

- 现有的方法通常求助于节点嵌入和使用递归神经网络(RNN,一般来说)动态调节嵌入和学习。这些方法需要一个节点的知识在整个时间跨度(包括训练和测试)和不太适用于频繁变化的节点集。本文提出了EvolveGCN沿着时间维度调整图卷积网络（GCN）模型，而不求助于节点的嵌入。

- EvolveGCN通过使用一个循环模型来演化GCN参数来捕捉图序的动态性。

  ![image-20221105133856438](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221105133856438.png)

  图中的RNN是典型的GRU或者LSTM。提出了两种进化GCN权重的方案EvolveGCN-H版本和EvolveGCN-O版本。

- 然后，在时间步骤t，输入数据由一对$(A_t∈R^{n×n},X_t∈R^{n×d})$组成，其中前者是图的（加权）邻接矩阵，后者是输入的节点特征矩阵，其中每一行的$X_t$是一个d维的节点特征向量。

- GCN的定义如下：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221105134336092.png" alt="image-20221105134336092" style="zoom:80%;" /><img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221105134344248.png" alt="image-20221105134344248" style="zoom:80%;" />

- GCN的权重演变依靠GRU或者LSTM：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221105134456774.png" alt="image-20221105134456774" style="zoom:80%;" />

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221105134509787.png" alt="image-20221105134509787" style="zoom:80%;" />

### Time-Series Event Prediction with Evolutionary State Graph

WSDM2021

- 对时间序列数据中的未来事件进行准确和可解释的预测，往往需要捕捉支撑观察数据的代表性模式（或称为状态）。为此，大多数现有的研究集中在状态的表示和识别上，但忽略了它们之间的变化过渡关系。本文提出了进化状态图，这是一种动态图结构，旨在系统地表示状态（节点）之间随时间演变的关系（边）。我们对从时间序列数据中构建的动态图进行了分析，并表明图结构的变化（例如，连接某些状态节点的边）可以告知事件的发生（即时间序列的波动）。受此启发提出了一种新型的图神经网络模型--进化状态图网络（EvoNet），对进化状态图进行编码，以实现准确和可解释的时间序列事件预测。具体来说，进化状态图网络对节点级（状态到状态）和图级（段到段）的传播进行建模，并捕捉到节点-图（状态到段）随时间变化的相互作用。基于五个真实世界数据集的实验结果表明，与11个baseline相比，不仅取得了明显的改进，而且还为解释事件预测的结果提供了更多的见解。

- 时间序列可以被分割并识别为几个状态（例如，1-4）。在此基础上，我们构建了进化状态图，其中每个节点表示一个状态，边表示它们在相邻段的过渡关系。在此基础上，我们开发了EvoNet，以进一步捕捉有效的时间序列事件预测的重要模式。

  ![image-20221106164622327](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221106164622327.png)

- GCN-LSTM需要一个明确的图作为输入（例如，应用内的行动图），这很难从一般的时间序列数据中直接得到。Time2Graph使用shapelets来发现状态和关系，但它只计算整个时间线上的单一静态图，尽管状态关系可能会随着时间的推移而改变（例如，节点级的动态和图级的迁移，详见3.1节）。据我们所知，现有的研究都没有成功地捕捉和模拟时间序列状态之间的时间变化关系。

### Link Prediction with Spatial and Temporal Consistency in Dynamic Networks

IJCAI2017

- 动态网络是无处不在的。动态网络中的链接预测已经引起了巨大的研究兴趣。有两个关键因素:1）一个节点更有可能在不久的将来与它附近的另一个节点形成链接，而不是与一个随机节点形成链接；2）动态网络通常是平滑演化的。现有的方法很少将这两个因素统一起来，以争取动态网络的空间和时间上的一致性。为了解决这一局限性，本文提出了一个具有空间和时间一致性的链接预测模型（LIST），用于预测一连串网络中随时间变化的链接。LIST将网络动态描述为时间的函数，它整合了每个时间戳的网络空间拓扑结构和时间上的网络演化。与现有方法相比，LIST有两个优点。1）LIST使用一个通用模型来表达作为时间函数的网络结构，这使得它也适用于本文重点以外的各种时间网络分析问题；2）通过保留空间和时间的一致性，LIST产生了更好的预测性能。在四个真实数据集上进行的广泛实验证明了LIST模型的有效性。

### Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN

- 网络嵌入的目的是学习节点的低维表示，同时捕捉网络的结构信息。它在网络分析的许多任务上取得了巨大的成功，如链接预测和节点分类。现有的网络嵌入算法大多集中在如何有效地学习静态同质网络。然而，现实世界中的网络更加复杂，例如，网络可能由几种类型的节点和边缘组成（称为异质信息），并且可能随着时间的推移，动态节点和边缘的变化（称为进化模式）。对于动态异质网络的网络嵌入，人们做了有限的工作，因为同时学习进化和异质信息是很有挑战性的。本文提出了一种新的动态异质网络嵌入方法，称为DyHATR，它使用分层注意力机制来学习异质信息，并将递归神经网络与时间注意结合起来以捕捉进化模式。在四个真实世界的数据集上对我们的方法进行基准测试，以完成链接预测的任务。实验结果表明，DyHATR明显优于几个最先进的baseline。

- 动态异质网络嵌入的主要任务是同时捕捉动态网络上的异质信息和时间演化模式。本文提出了一种新的动态异质网络嵌入方法，该方法使用层次注意模型来捕捉快照的异质性，使用时间注意RNN模型来学习进化时间上的进化模式。

  ![image-20221106203647903](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221106203647903.png)

  DyHATR模型包含三个部分，(a)分层注意模型，(b)时序注意RNN模型和(c)动态连接预测。右边的柱状图显示了DyHATR在EComm数据集上比DeepWalk、metapath2vec和metapath2vec-GRU的性能改进。

- 分层注意力模型：分层注意模型通过将异质快照按照不同的边缘类型分割成几个特定类型的子网络来捕捉静态快照的异质性信息。分层注意模型包含两个部分，节点级注意和边缘级注意。

  - 节点级的注意力模型旨在学习每个节点邻域的重要性权重，并通过聚合这些重要邻域的特征来生成新的潜在表征。对于每个静态异质快照$G_t∈G$，我们对每个具有相同边缘类型的子图采用注意力模型。边缘类型r和第t个快照的节点对（i，j）的权重系数可以计算为:

    <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221106203948052.png" alt="image-20221106203948052" style="zoom:80%;" />

    然后，将邻居的潜伏嵌入与计算出的权重系数汇总，可以得到节点i在边缘类型r和第t个快照的最终表示为：

    <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221106204046791.png" alt="image-20221106204046791" style="zoom:80%;" />

    因此,节点的多头注意力机制在边缘类型r和t快照可以被描述为：

    <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221106204132310.png" alt="image-20221106204132310" style="zoom:80%;" />

  - 节点级的注意模型可以捕获单一边缘类型的特定信息。然而，异质网络通常包含多种类型的边。为了整合每个节点的多种边缘特定信息，采用边缘级注意力模型来学习不同类型边缘的重要性权重，并汇总这些不同类型的特定信息来生成新的嵌入。

    首先，将特定的边缘嵌入输入非线性转换函数，以映射到相同的特征空间$σ(W - h^{rt}_i +  b)$，其中σ是激活函数（即tanh函数）；W和b分别是可学习的权重矩阵和偏置矢量。这两个参数在所有不同的边缘类型和快照中都是共享的。然后，通过计算映射的边缘特定嵌入和边缘级注意力参数化向量q之间的相似性来衡量输入的边缘特定嵌入的重要性系数。之后就可以汇总这些边缘特定的嵌入来生成节点i在第t个快照的最终表示特征：

    <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221106204335528.png" alt="image-20221106204335528" style="zoom:80%;" />

- 时序注意RNN模型：时间上的演化模式显示了随着时间的推移，节点和边的出现和消失。DyHATR的层次注意模型可以有效地捕捉静态快照的异质性，但它不能模拟随时间演变的模式。最近，引入循环神经网络（RNN）的动态网络嵌入方法取得了良好的性能。在本文中扩展了现有的RNN模型，并提出了一个时间关注的RNN模型，以捕捉连续时间戳之间更深层次的演变模式。所提出的时态关注RNN模型主要包含两部分：递归神经网络和时态自关注模型。

  - LSTM：

    <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221106204803255.png" alt="image-20221106204803255" style="zoom:80%;" />

  - GRU：

    <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221106204819281.png" alt="image-20221106204819281" style="zoom:80%;" />

  - 本文的模型中，在RNN模型的输出上采用了一个时间层面的注意力模型来捕捉重要的特征向量。没有把所有的特征向量连接在一起作为最终的嵌入来预测最后一个快照中的动态链接，而是采用了一个时间层面的自我关注模型来进一步捕捉动态网络上的演化模式。从RNN模型的输出中，可以得到节点i在不同快照下的嵌入：<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221106205030899.png" alt="image-20221106205030899" style="zoom:80%;" />，其中D是输入嵌入的维度。假设时间层次注意力模型的输出是<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221106205055562.png" alt="image-20221106205055562" style="zoom:80%;" />，其中D′是输出嵌入的维度。缩放点注意力机制被用来学习每个节点在不同快照下的嵌入，时序级注意力模型被定义为：

    <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221106205156819.png" alt="image-20221106205156819" style="zoom:80%;" />

    至此获得了所有快照中节点i的所有嵌入，但将使用最后一个快照的嵌入，表示为$z^T_i$，用于优化和下游任务。

- DyHATR的目的是捕捉动态异质网络中的异质信息和演化模式，定义的目标函数是利用二元交叉熵函数来确保最后快照的节点u与其附近的节点有相似的嵌入特征。DyHATR的损失函数定义为：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221106205400107.png" alt="image-20221106205400107" style="zoom:80%;" />

### Node Embedding over Temporal Graphs

IJCAI19

- 本文提出了一种在时序图中进行节点嵌入的方法，可以学习时序图的节点和边随时间的演变模式，并将这种动态纳入不同图预测任务的时态节点嵌入框架中。提出了一个联合损失函数，通过学习结合节点的历史时间嵌入来创建节点的时间嵌入，从而优化每个给定的任务（例如连接预测）。该算法使用静态节点嵌入进行初始化，然后在不同时间点的节点表征上进行排列，并最终在联合优化中适应给定的任务。评估了本文方法在各种时间图上的有效性，用于时序连接预测和多标签节点分类这两项基本任务，并与有竞争力的基线和替代算法进行了比较。本文算法在许多数据集和基线上都显示出性能的提高，并且发现对凝聚力较低、聚类系数较低的图特别有效。

- 节点分类任务的交叉熵损失函数：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221106214341254.png" alt="image-20221106214341254" style="zoom:80%;" />

  连接预测任务的损失函数：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221106214447238.png" alt="image-20221106214447238" style="zoom:80%;" />

- 我们希望利用图形快照集G1,...,GT来学习一个函数$F_T$，即：$f_T（v）=F_T（v,G_1,...,G_T）$，能对$L_task$来说是最优化的。为了学习节点动态，将节点v在时间t+1的嵌入形式化为一个递归表示法：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221106214957934.png" alt="image-20221106214957934" style="zoom:80%;" />

  其中$f_0(v)=~0$，$A,B,R_t$和$Q_t$是在训练中学习的矩阵，v是代表一个节点的one-hot向量，σ是一个激活函数。

  将最终的时序嵌入学习问题表述为一个优化问题，即最小化以下损失函数：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221106215152974.png" alt="image-20221106215152974" style="zoom:80%;" />

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221106215235112.png" alt="image-20221106215235112" style="zoom:80%;" />

### TREND: TempoRal Event and Node Dynamics for Graph Representation Learning

www2022

- 

### *APAN: Asynchronous Propagation Attention Network for Real-time Temporal Graph Embedding

- 为了捕捉更高层次的结构特征，大多数基于GNN的算法学习了包含k-hop邻居信息的节点表示。由于查询k-hop邻居的时间复杂度很高，大多数图算法不能部署在一个巨大的密集时空网络中，以执行毫秒级的推理。这个问题极大地限制了图算法在某些领域的应用潜力，特别是金融欺诈检测。因此本文提出了异步传播注意网络（APAN），一种用于实时时态图嵌入的异步连续时间动态图算法。传统的图模型通常执行两个串行操作：首先是图查询，然后是模型推理。与以往的图算法不同，APAN将模型推理和图计算解耦，以减轻繁重的图查询操作对模型推理速度的损害。大量的实验表明，所提出的方法在大大提高推理速度的同时，还能达到有竞争力的性能。

- 除了对金融犯罪的快速反应，在其他优势领域应用CTDG算法也面临着各种技术问题。a）当交互频率在短时间内突然增加，如黑色星期五，图数据库将过载，整个平台会不稳定。b）的确CTDG算法有潜力捕捉节点和图的快速变化，但如果因效率限制而不能在网络平台上部署，这种潜力就不能得到最大限度的发挥。

- APAN有两个环节：同步推理环节和异步传播环节。

  在异步传播环节，一旦交互完成，交互的详细信息将以 "邮件 "的形式传递到其k跳邻居的 "邮箱  "中。在时间嵌入生成后，邮件传播器首先创建一个邮件，然后沿着时间边缘将其传播到k-hob邻居的邮箱中。该邮件涵盖了与发送节点相关的交互，由于邮件传播器处于异步环节，不会损害用户体验，可以在这个模块中处理一些更复杂的计算，如聚合来自多层的邻居或计算子图的统计值。

  在同步传播环节，当交互发生时，APAN不需要查询时空图中的邻居，而只需读出相关节点的 "邮箱"，代替生成实时推理。如果一个节点在一个批次中涉及几个互动，嵌入将只生成一次。所有参与新事件的节点的时间嵌入需要实时更新，所以这种操作促进了模型对图动态的捕捉。之后，MLP解码器将利用这些更新的节点嵌入来实现下游的任务，如链接预测、节点分类、边缘分类和节点集群。由于编码器和解码器都是前馈神经网络，而且不需要查询图数据库中的图邻居，所以完成这两个阶段的工作会非常快。

- APAN实现了推理速度的大幅提高，并具有竞争力的性能。除了高推理速度和高性能，异步传播机制还为APAN带来了更多有趣的好处：a）由于交互的详细信息被存储在邮箱中，APAN有可能成为一个可解释的模型。(第3.6节） b) APAN克服了传统CTDG算法的共同缺点--对批量大小的敏感性。(第4.7节） c）在某些任务中，提高推理速度也能有效地提高商业价值。

- ![image-20221105212049119](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221105212049119.png)

  异步传播注意网络（APAN）的整体框架主要可分为三个部分：编码器、解码器和传播器。编码器和解码器处于同步链路中，它们不需要从图数据库中查询邻居的信息。因此，从交互发生到模型推断的时间延迟将非常短，用户将获得非常流畅的体验。在模型推断之后，邮件传播器会根据交互生成邮件，然后沿着时空边缘传播到k-hop邻居的邮箱中。

- <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221105212614773.png" alt="image-20221105212614773" style="zoom:80%;" />

  - APAN的编码器由多头注意力机制模块组成，根据根据上一次更新的embedding$z(t-)$以及邮箱内容$M(t)$之间的相关性来生成当前节点embedding$z(t)$。其中邮箱内容还结合了position encoding：

    <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221105212929854.png" alt="image-20221105212929854" style="zoom:80%;" />

    在实践中，注意力模型总是采用多个注意力头来形成多个子空间，强制模型学习不同方面的信息。多头注意力机制：

    <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221105212953713.png" alt="image-20221105212953713" style="zoom:80%;" /><img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221105213006107.png" alt="image-20221105213006107" style="zoom:80%;" />

    由于不同节点的注意力输出是不同的，需要一个归一化方案来限制输出的平均值和方差。Layer Normalization通过计算用于归一化的平均数和方差来实现这一目标，这些平均数和方差来自层中神经元的所有输入之和：

    <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221105213130079.png" alt="image-20221105213130079" style="zoom:80%;" />

    𝑑表示层归一化输入的维度，𝜇和𝜎 是均值和方差，由一个层中的所有隐藏单元所共享。⊙是两个向量之间的元素相乘。可学习参数b和g被定义为偏置和增益，以确保规范化操作对原始信息没有影响。

## 最新论文

### An Explainer for Temporal Graph Neural Networks

- 本文提出了一个新颖的TGNN模型的解释者框架，给定要解释的图的时间序列，该框架可以在一个时间段内以概率图模型的形式识别主导的解释。对交通领域的案例研究表明，所提出的方法可以发现一个时间段内道路网络的动态依赖结构。
- <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221103165947093.png" alt="image-20221103165947093" style="zoom:67%;" />

### Interpretable Clustering on Dynamic Graphs with Recurrent Graph Neural Networks

- 图聚类算法

### Line Graph Contrastive Learning for Link Prediction

- 链接预测任务旨在预测网络中两个节点的连接情况，现有的工作主要通过节点对的相似性测量来预测链接，然而，如果局部结构不符合这种测量假设，算法的性能将迅速恶化。为了克服这些限制，本文提出了一种线图对比学习（LGCL）方法来获得多视图信息。该框架通过以目标节点对为中心的h-hop子图采样获得子图视图。将采样的子图转化为线图后，可以直接获取边缘嵌入信息，并将链接预测任务转化为节点分类任务。然后，不同的图卷积算子从双重角度学习表示。最后，采用对比学习法，通过最大化相互信息来平衡这些视角的子图表示。通过对六个公共数据集的实验，LGCL在链接预测任务上的表现优于目前的baseline，并显示出更好的泛化性能和稳健性。![image-20221103222019570](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221103222019570.png)

### *Hyperbolic Variational Graph Neural Network for Modeling Dynamic Graphs

AAAI2021

- 学习图的表征在广泛的下游应用中起着关键作用。本文三个方面总结了先前工作的局限性：表示空间、动态建模和不确定性建模。为了弥补这一差距，本文首次提出在双曲空间学习动态图表示，旨在推断随机的节点表示。利用双曲空间提出了一个新颖的双曲变异图神经网络，简称HVGNN。特别地，为了对动态进行建模，引入了一个基于理论基础的时间编码方法的时态GNN（TGNN）。为了对不确定性进行建模，设计了一个建立在拟议的TGNN基础上的双曲图变异自动编码器，以生成双曲正态分布的随机节点表示。此外为双曲正态分布引入了一种可重新参数化的采样算法，以实现HVGNN的基于梯度的学习。大量的实验表明，HVGNN在真实世界的数据集上的表现优于最新的baseline。

- 现有方法的三个局限：

  - Representation Space：在欧几里得空间中对图进行建模。欧几里得模型在表示具有潜在层次的现实世界的图时，往往会出现失真。且欧几里得空间大小以多项式形式增长。
  - Modeling Dynamics：忽视图的内在动态性通常会导致有问题的推断。这样的模型可能会错误地利用未来的信息来预测过去的互动，因为不断演变的约束被忽略了。
  - Modeling Uncertainty：**图的形成和演化充满了不确定性，特别是对于低度节点，它们传递的信息更少，承担的不确定性更大。**实际上，不确定性是图的一个固有特征。确定性的表示法不能对不确定性进行建模。另外，随机表示提供了一种有希望的方法来模拟这种特性。它自然地捕捉到了不确定性，将节点表示为正态分布，即平均值和方差。

- 为此本文提出了一个新颖的双曲变异图神经网络，简称HVGNN。在HVGNN中，为了解决第一个限制，利用双曲空间作为表示空间，而不是欧几里德空间。为了解决第二个限制，引入了一个新的时态GNN（TGNN）来模拟动态。特别是，TGNN在有理论基础的时间编码方法的基础上进行了时间意识的关注，该方法区分了时间域中的节点。为了解决第三个限制，设计了一个建立在TGNN基础上的双曲图变异自动编码器，以联合建模不确定性和动态性。

- <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221103215011667.png" alt="image-20221103215011667" style="zoom:67%;" /><img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221103223817060.png" alt="image-20221103223817060" style="zoom:67%;" />

  除了图上的attention，HYPTGA层还进一步整合了时间编码φL(-)，这样就可以在时域中区分邻域来建立图的动态模型，关注的是邻居之间的相对时间模式。

### *TPGNN: Learning High-order Information in Dynamic Graphs via Temporal Propagation

- 本文的目标是解决一个重要但被忽视的问题：即如何从时空图中的**高阶邻居**那里学习信息，以提高所学节点表征的信息量和辨别力。在学习高阶信息时会面临两个挑战：**无效计算和过平滑**，无法用静态图的方法解决。

- 本文方法TPGNN由两个组件构成：传播器（propagator）和节点级编码器（node-wise encoder）。传播器被用来将信息从锚节点传播到其k-hop范围内的时空邻居，然后同时更新邻居的状态（迭代更新每个节点的状态，而不混合来自不同层的信息），这使得高效的计算成为可能，特别是对于一个深度模型。此外，为了防止过度平滑，该模型强制来自n-hop邻居的消息更新保留在锚上的n-hop记忆向量。节点级编码器采用transformer的结构，通过明确学习保留在节点本身上的记忆向量的重要性来学习节点表征，即隐式地对来自不同层的邻居的消息的重要性进行建模，从而减轻了过度平滑的情况。由于编码过程不会查询时间上的邻居，我们可以极大地节省推理中的时间消耗。在时间链路预测和节点分类方面的大量实验表明，TPGNN在效率和鲁棒性方面优于最先进的baseline。

- 标准的CTDG方法TGAT主要处理包括两个阶段：（1）从一批交互中生成一个子图，（2）递归地进行带有时间编码的图卷积以学习节点表征。

  由于**子图的生成（即第一阶段）对所有基于CTDG的算法都是必要的**，本文认为计算的低效率来自于图卷积的聚合（即第二阶段）。具体来说，这是因为**vanilla graph**卷积中的更新操作不能被并行化到计算图中的所有节点。除非对高阶邻居的计算已经完成，否则消息无法传递给低阶节点，当模型很深时，这种情况尤其严重，因为计算图会呈指数级扩张。此外，由于必须保存消息传递的中间状态，这些方法也有沉重的内存负担。

- ![image-20221103193243225](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221103193243225.png)首先从时空图中抽取一批边来形成子图，然后利用节点编码器来计算节点表征。然后，学习到的表征同时被送入传播器以更新节点状态，并被送入解码器以执行下游任务（例如，链接预测)。由于TPGNN有一个具有k层的模型，时空图中的每个节点都保留了k个记忆，每个记忆都通过相应层的信息进行更新。例如，1-跳的记忆是通过1-跳邻居的消息来更新的。

- 传播器用于将目标节点产生的消息传播到消息传递路径上的邻域。与现有算法（如TGN）相比，传播器的使用带来了三个好处：(i) 对于每次交互，可以更新消息传递中所有节点的状态，而不是只更新目标节点的表征，这证实了我们可以保留更多的时间性并防止信息丢失。(ii) 每层的更新过程都是单独的，因此可以先将消息传播给邻居，然后并行地更新每个邻居的状态，这保证了高效的计算。(iii) 由于我们不需要在TGN中保存聚合的中间结果，所以不会造成很大的内存消耗负担。

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221103193816135.png" alt="image-20221103193816135" style="zoom: 80%;" />

  如上图所示，用橙色表示锚节点，用灰色表示受影响的节点。在信息从锚节点产生后，将信息传播给它的k-hop邻居，在这个例子中，k=2。然后同时更新保存在各层邻居上的记忆。具体来说，如果受影响的节点是锚的直接邻居（即1跳），那么受影响节点上的1跳记忆可以被消息更新。当2个消息被传递到一个节点时，首先在更新前合并消息。

- 节点式编码器的结构如下图所示。该组件意在带来两个好处：(1)学习节点本身保留的所有记忆的重要性，这可以减轻过度平滑；(2)在不查询时间邻域的情况下进行编码，这可以大大减少推理中的时间消耗。节点式编码器中的变换器可以多层堆叠，以进一步提高模型能力和表现力。

  ![image-20221103194858350](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221103194858350.png)

### *Provably expressive temporal graph networks

NIPS2022

- 动态图神经网络方法（Temporal graph networks, TGNs）很少有以它们的理论基础出名的，本文建立了两个主要类别tgn（Temporal graph networks）：聚合随机游走的方法(WA-TGNs)，和通过周期性记忆组件进行消息传递的方法(MP-TGNs)，他们的基本的表征能力和限制。本文通过从理论上分析几种TGN方法的局限，来设计更为强大、理论支撑充分、实际有效的模型。

  #### 1.Introduction

- 用WL-test进行测试，发现第二类方法(MP-TGNs)中使用的是单射更新，这种方法无法发区分一些图，它们的表征能力和WL-test相同。

- memory的作用：只有一些层的网络中的节点无法通过消息传递从充分广阔的接受域（如距离很远的节点）聚合信息，所以memory通过附加的全局性信息来补偿这种局限。作为对比，足够深层次的网络结构排除了对于memory的需求。

- WA-TGNs和MP-TGNs都没有比对方更具表征能力，它们彼此互补了对方无效的场景。但它们无法判断图的一些属性例如直径、周长、cycle的数目等等。

- PINT同时利用了两种方法的优势，它使用了单射的消息传递和更新步骤，并且给memory增加了相关的位置特征，这些特征复制了WA-TGNs方法的优势。并且PINT的训练速度要快于CAW，证明了PINT比WA-TGNs和MP-TGNs都更具表征能力。

- 本文的贡献有三方面：

  - 提出对于TGN原理的严密的理论证明，阐述了memory的作用、单射消息传递的优势、现有的TGN模型的局限性、对于1-WL在时序上的扩展和它的蕴含信息、时序图性能的局限、不同类TGN方法之间的关系。
  - 引入了一种清楚明确的单射时序函数以及更为新颖的时序图方法，被证明比SOTA更具表征能力。
  - 大量的实验调查来强调本方法的实际优势。

  #### 2、3The representational power and limits of TGNs

- 研究MP-TGNs方法，先假定MP-TGNs融合的是全部邻居的信息。MP-TGNs的缺陷在于它不能区分一些简单的时序图。记它们的时序计算树为temporal computation tree (TCT)。

- ![image-20221114105547655](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221114105547655.png)

  如上图所示，节点u和v的TCT是不同构的。但TGAT/TGN-Att这类基于消息传递的方法的注意力层会为u和v传递一样的消息，因为TGAT/TGN-Att的注意层在相同的多组数值上计算加权平均数，导致MP-TGNs方法无法区分事件<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221103110442016.png" alt="image-20221103110442016" style="zoom:67%;" />，因为节点z和u的TCT是同构的；而CAW无法区分事件<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221103110535895.png" alt="image-20221103110535895" style="zoom:67%;" />，节点u和u‘深度为3的TCT是不同构的，但长度为1的walk阻止了CAW捕获这种结构上的差异。

- ![image-20221103110821453](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221103110821453.png)

  当MP-TGNs方法的层数L小于图的直径时，带有memory的方法比不带memory的方法更能区分一些节点。对于任意层数L，具有L+直径的层数的方法至少和带有memory的方法具备相同的区分节点的性能。

- 一些时序图在直径、周长、cycle的个数等属性上有所不同，但MP-TGNs方法和CAW都无法区分。

  ![image-20221103111224662](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221103111224662.png)

  G(t)和G'(t)直径不同（正无穷和3），周长不同（3和6），cycle的数量不同（2和1）。对于G(t)中的任意一个节点，在G'(t)中都存在节点和它的TCT是同构的，因此对于它们的embedding将会是相同的。

- ![image-20221103112233504](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221103112233504.png)

  PINT沿用了消息传递机制来更新memory，并增加了为节点计算的位置特征$r^{(t)}_{j\rightarrow u}$，表示从节点u在事件时间t的TCT中节点j的位置特征向量，其中向量中的第k项$r^{(t)}_{j\rightarrow u}[k]$表示在时间t，从u到j在k步内有多少条不同的路径。

  PINT最后将memory state和位置特征向量concat起来输入MLP进行连接预测。

- 但PINT也有失效的场景。

  ![image-20221103113009523](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221103113009523.png)

  如上图所示，当$(u,v,t_3)$和$(v,z,t_3)$是两个同时发生的事件时，PINT无法区分两者。CAW也无法区分。

### Temporal Graph Network Embedding with Causal Anonymous Walks Representations

- 本文将TGN和CAW结合，

- 图机器学习中的许多任务，如链接预测和节点分类，通常是通过使用表示学习来解决的，其中网络中的每个节点或边缘都通过嵌入来编码。尽管存在大量针对静态图的网络嵌入，但在分析动态（即时间）网络时，任务变得更加复杂。本文将TGN和CAW结合，提出了一种基于时态图网络的动态网络表示学习的新方法，通过提取因果匿名漫步，使用高度定制的消息生成函数。

  为了评估，提供了一个基准管道来评估时态网络嵌入。这项工作为涉及节点分类和链接预测的图机器学习问题，在每一个可用的设置中提供了第一个全面的时态网络表示学习的比较框架。提出的模型优于最先进的baseline模型。这项工作也证明了他们之间的差异，基于在各种归纳/归纳边缘/节点分类任务中的评估。此外，展示了本文的模型在现实世界的下游图机器学习任务中的适用性和优越性能，该任务由欧洲顶级银行之一提供，涉及基于交易数据的信用评分。

- <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221107192611086.png" alt="image-20221107192611086" style="zoom: 67%;" />

- <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221107193127445.png" alt="image-20221107193127445" style="zoom: 50%;" />

### Neural Predicting Higher-order Patterns in Temporal Networks

- 最新发现涉及多个交互节点的高阶模式对于表明不同时空网络的特定领域规律至关重要，这带来了挑战，即为这些高阶模式和相关的新学习算法设计更复杂的超图模型。本文提出了第一个模型HIT，用于预测时空超图中的全谱高阶模式。特别专注于预测三种常见但重要的互动模式，涉及时态网络中的三个互动元素，这可以扩展到更高阶的模式。HIT提取了时空超图上感兴趣的节点三联体的结构表示，并利用它来告知在这个三联体中可能发生什么类型、什么时候和为什么会发生互动扩展。此外，HIT通过识别时间超图上最具辨别力的结构特征来预测不同的高阶模式，提供了一定程度的可解释性。
- ![image-20221103205525229](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221103205525229.png)

### *TIGER: Temporal Interaction Graph Embedding with Restarts

WWW2023

- 时序交互图（TIGs）由有时间戳的交互事件序列组成，在电子商务和社交网络等领域很普遍。为了更好地学习随时间变化的动态节点嵌入，研究人员提出了一系列用于TIGs的时态图神经网络。然而，由于entangled的时间和结构依赖性，现有的方法必须按时间顺序连续处理事件序列，以确保节点表示是最新的。这使得现有的模型无法并行化，并降低了它们在工业应用中的灵活性。

- 为了解决上述挑战，本文提出了TIGER，一个可以在任何时间戳重新启动的TIG嵌入模型。本文引入了一个重启模块，该模块生成的surrogate representations作为节点表示的warm initialization。通过同时从多个时间戳重启，将序列分为多个块，并自然地实现了模型的并行化。此外，与之前利用单一内存单元的模型相比，TIGER引入了双内存模块，以更好地利用邻域信息并缓解staleness问题。本文在四个公共数据集和一个工业数据集上进行了广泛的实验，结果验证了我们工作的有效性和效率。

- 许多现实世界的系统可以被表述为时空交互图（TIGs），包括对象（即节点）之间有时间戳的交互事件序列，即边。与静态图表示学习不同，时态交互图嵌入的目的是同时编码时间和结构依赖，并产生适合各种下游任务的节点表示。早期对时空交互图嵌入的尝试主要集中在生成静态表示上。然而，这些方法不能处理图形演变过程中的新节点或边。另一方面，对于真实场景中的时间感知预测任务来说，静态表示是不够的。例如，我们可以根据在线交易网络中一个用户最近的交易情况来预测他/她是否有欺诈行为，或者在推荐系统中，一个具有不断变化的欲望的用户是否会在未来购买某种物品。为了捕捉TIGs的动态性质并学习时间感知的表征，研究人员提出了时间图神经网络（GNNs），它可以联合模拟时间和结构的依赖性，并对新的节点和边进行泛化。**作为大多数时空图神经网络的一个关键要素，memory模块存储节点状态，代表节点的过去或历史信息。这使得时态GNN能够记忆长期的依赖关系，从而达到比无记忆模型更高的性能。**

- 这些模型存在着两个重要的问题：

  - 由于时态交互图的复杂依赖性，以前的方法不容易被并行化。与静态图不同，在时态GNN中，消息传递程序必须符合时间限制：一个节点不能聚合来自未来邻居的信息。另一方面，与可以并行处理多个独立序列的个性化序列模型不同，在时态GNN中，所有节点的事件都是纠结的。结构和时间依赖性的结合要求模型按时间顺序连续地遍历过去的边，以便记忆能够保持最新的状态。这严重降低了工业场景中应用的灵活性。考虑一个在线支付平台的案例，由于数据中心迁移等原因，已部署的模型在本月第一天的𝑡𝑏暂停，而在月底，我们希望它恢复服务。然后，需要处理模型离线期间发生的数十亿笔交易，如下图所示：

    <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20230214151432313.png" alt="image-20230214151432313" style="zoom:67%;" />

    该图是有和没有重启的方法的比较。(a) 以前的方法需要消耗离线数据来计算最新的内存，然后再返回服务。(b) 在restarter的帮助下，本文提出的方法可以通过估计的内存快速返回服务。(c) restarter也使提出的方法能够并行运行。

  - 以前的方法的另一个缺点是所谓的staleness问题。由于一个节点的memory只有在涉及该节点的事件发生时才会被更新，如果它有一段时间不活动，它的状态就会变得陈旧，从而导致零星不变的时间嵌入。为了处理这个问题，研究人员提出了时间嵌入模块。例如，TGN作为最具代表性的时态GNN之一，引入了时态图注意层，以聚合节点邻居的信息，因为一些邻居可能最近被更新。然而，通过分析数据流可以发现，由于这些方法的双分支结构，内存更新模式仍然存在滞后性问题。具有时间嵌入模块的分支确实可以生成新鲜的表示。但负责memory更新的分支完全忽略了这些新鲜信息，导致memory陈旧。

- ![image-20230214152722656](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20230214152722656.png)

  本文通过提出一个新的时态GNN框架TIGER（Temporal Interaction Graph Embedding with  Restarts）来解决上述问题。在TIGER中，引入了一个名为restarter的新模块。这个重启器被训练成模仿内存，可以在任何时候以可忽略不计的成本推断出内存状态。然后，restarter的输出可以作为内存的warm initialization，这样就可以在任何时候灵活地重新启动模型，引入restarter的另一个好处是，我们可以通过将事件序列分为多个块来并行运行我们的模型，这进一步提高了本文方法的灵活性和可扩展性。为了处理staleness问题，提出了一个双内存模块。双重内存模块将内存更新分支和时间嵌入分支统一起来，使时间嵌入模块产生的新鲜表征得到充分利用，并反馈给内存。此外明确区分了事件发生前和事件发生后的节点表征，并将它们存储在相应的存储单元中。这使得memory模块比以前的方法更容易解释。

  为了使梯度通过状态更新器和消息生成器，需要在时间𝑡 ′′上使用倒数第二个事件。(a)  TGN只包含一个存储单元M，时间嵌入模块的输出在计算损失L1后被丢弃。(b)  TIGER引入了双存储器来跟踪事件前后的节点表示。与TGN相比，TIGER对内存中的数值有明确的含义，并充分利用了时间嵌入模块的输出。(c) 在TIGER中，restarter在训练过程中偶尔会重新初始化内存。restarter的训练目标是模仿编码器的输出。



### Neighborhood-aware Scalable Temporal Network Representation Learning

- 图神经网络（GNN）已被广泛用于编码网络结构化数据，并在许多任务中取得了最先进的（SOTA）性能，如节点/图分类。然而，要预测时态网络中的节点如何相互作用，GNNs的直接概括可能效果不佳。传统的GNN通常为每个节点学习一个向量表示，并根据两个向量表示的组合（如内积）来预测两个节点是否可能相互作用（又称链接）。这种链接预测策略往往不能捕捉到两个节点的联合邻域的结构特征。

- 考虑下图中的时态网络的玩具例子：节点w和节点v在t3之前有相同的局部结构，所以GNN包括其在时态网络上的变体（如TGN）将把w和v与相同的向量表示联系起来。因此，GNNs将无法做出正确的预测，以判断u在t3时将与w还是v发生交互。这里，GNNs不能捕捉到重要的联合结构特征，即u和v在t3之前有一个共同的邻居a。这个问题使得以前几乎所有将GNNs泛化为时态网络的工作都只能提供不合格的性能：

  ![image-20230313145716077](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20230313145716077.png)

- 如果这是一个社会网络，（u，v）很可能发生，因为u，v有一个共同的邻居a，并且遵循三元封闭的原则[2]。然而，传统的GNN，即使是在时间网络上的泛化，在这里也是失败的，因为由于节点v和节点w有共同的结构背景，他们学习了相同的表示，如中间所示。在右边，我们展示了基于u和v的N-缓存的联合邻域特征的高级抽象：在节点u和节点v的1跳邻域的N-缓存中，a作为键出现。将这些键连接起来可以提供一个结构性的特征，至少对于预测来说，可以编码这种共同邻域的信息。

- 在这项工作中提出了邻里感知时态网络模型（NAT），它可以解决上述建模问题，同时保持模型的良好可扩展性。NAT的关键创新之处在于用字典型邻域表示法代替单矢量节点表示法，并采用计算友好的邻域缓存（N-cache）来维护这种字典型表示法。具体来说，一个节点的N-缓存在GPU上存储几个大小受限的字典。每个字典都有一个中心节点的历史邻居的采样集合作为键，并将时间戳和连接到这些邻居的链接上的特征汇总为值（矢量表示）。有了N个缓存，NAT可以并行地构建一批节点对的联合邻域结构特征，以实现快速链接预测。通过采用支持GPU并行计算的基于哈希的搜索函数，NAT还可以用新的交互邻域有效地更新N-缓存。
  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20230313150100612.png" alt="image-20230313150100612" style="zoom:67%;" />

  如上图所示，当u和v在$t_3$时刻发生交互时，u的0-hop表征（也就是它自己本身的表征）、它的1-hop表征会基于节点v本身的特征来进行更新，这个更新也是通过RNN实现的。u的2-hop表征字典会将v的1-hop邻居插入。以此类推更新节点u的k-hop邻居字典，该字典存储在N-cache中。

### *Self-Supervised Temporal Graph learning with Temporal and Structural Intensity Alignment

- 时态图学习的目的是为基于图的任务生成高质量的表征以及动态信息，这一点最近已经引起了越来越多的关注。与静态图不同，时态图通常以连续时间的节点交互序列的形式组织，而不是邻接矩阵。大多数时间图学习方法通过结合一段时间的历史信息来模拟当前的互动。然而，这些方法仅仅考虑了一阶时间信息，而忽略了重要的高阶结构信息，导致了次优的性能。为了解决这个问题，通过提取时间和结构信息来学习更多的信息节点表征，我们提出了一种自监督的方法，称为S2T的时间图学习。请注意，**一阶时间信息和高阶结构信息被初始节点表征以不同的方式结合起来，分别计算出两个条件强度。**然后**引入对齐损失，通过缩小两个强度之间的差距来优化节点表征，使其信息量更大。**具体来说，**除了利用历史上的邻居序列对时间信息进行建模，我们还进一步考虑了局部和全局层面的结构信息。在局部水平上，我们通过聚合高阶邻居序列的特征来产生结构强度。在全局层面，基于所有节点生成一个全局表示，根据不同节点的活动状态调整结构强度。**广泛的实验表明，与最先进的竞争对手相比，所提出的S2T方法在几个数据集上实现了最多10.13%的性能改进。

- 由于时态图的特殊数据形式，即按时间排序的节点互动，时态方法是以成批的数据进行训练的。因此，这些方法通常在交互序列中存储邻居，并从历史信息中模拟未来的交互。然而，这种时间方法仅仅考虑了一阶的时间信息，而忽略了重要的高阶结构信息，导致了次优的性能。

- 为了解决这个问题，通过提取时间和结构信息来学习更多的信息节点表征，我们提出了一种自监督的方法，称为S2T的时间图学习。一阶时间信息和高阶结构信息以不同的方式被初始节点表征结合起来，分别计算出两个条件强度。然后引入对齐损失，通过缩小两个强度之间的差距来优化节点表征，使其信息量更大。

  更特别的是，对于时间信息建模，利用Hawkes过程来计算两个节点之间的时间强度。除了考虑两个节点的特征外，霍克斯过程还考虑了它们的历史邻居对它们未来互动的影响。

  另一方面，我们进一步提取结构信息，它可以分为局部和全局层面。在捕捉局部结构信息时，我们首先利用GNN，通过聚合高阶邻域特征来生成节点表征。之后，提取全局结构信息以增强长尾节点。特别是，我们提出了一个基于所有节点生成的全局表征，它被用来根据不同节点的活动状态更新节点表征。在基于节点表征计算出结构强度后，我们还构建了一个全局参数，为结构强度向量的不同维度分配重要性权重。

  最后，除了任务损失，我们还利用对齐损失来缩小时间和结构强度向量之间的差距，并对全局表示和参数施加约束。这构成了总的损失函数。

- <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20230216135518544.png" alt="image-20230216135518544" style="zoom:67%;" />

  如上图所示，S2T可以分为几个主要部分：时间信息建模、结构信息建模和损失函数，其中一个局部模块和一个全局模块共同实现结构信息建模。然后，在损失函数中引入对齐损失，对时间信息和结构信息进行对齐。

- **时间信息建模**

  给定两个节点x和y，可以通过计算它们之间的条件强度来表示它们相互作用的可能性。有两种方法可以获得，这里讨论第一种方法：用霍克斯过程对时间信息进行建模。这种点过程认为一个节点的历史邻居将影响该节点未来的互动，而且这种影响会随着时间的推移而衰减。节点间时间上的交互强度计算为：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20230216143607251.png" alt="image-20230216143607251" style="zoom:67%;" />

  这种强度可以分为两部分。第一部分是没有任何外部影响的两个节点之间的基础强度，即$μ_{(x,y)}$。第二部分是霍克斯强度，重点是一个节点的邻居如何影响另一个节点，其中$i$表示序列中的邻居。

  在霍克斯强度中，$α_{(i,y)}$衡量$x$的单个邻居节点$i$对$y$的影响，这种影响由两个方面加权。一方面，$s_{(i,x)}$是邻居序列中邻居$i$和源节点$x$之间的相似性权重，虽然我们计算$N_x$（过去时间的邻居序列）中每个邻居$i$对$y$的影响，但我们还需要考虑$N_x$中不同i的相应权重$s(i,x)$。因此在霍克斯强度中，$μ_{(i,y)}$和$μ_{(i,x)}$都会出现，而它们的作用是不同的。另一方面，函数$f(t_c-t_i)$考虑了$i$和$x$之间的互动时间间隔，即<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20230216143916631.png" alt="image-20230216143916631" style="zoom:67%;" />，其中$δ_t$是一个可学习参数。在这个函数中，与当前时间$t_c$更接近的邻居的互动被赋予更多的权重。此外，**邻居的总数可能因节点而异。在实际训练中，如果我们为每个节点获取其所有的邻居，每批的计算模式就不能固定，这带来了很大的计算不便。参考以前的工作和我们的实验，我们固定了节点邻居的序列长度$S$，在每个时间戳选择每个节点的最新$S$邻居，而不是全部邻居。**在第四节，我们将讨论实验中超参数$S$的敏感性。

- **局部结构信息建模**

  除了Hawkes过程之外，GNN模型也可以用来计算条件强度。与关注一阶邻居的时间信息的霍克斯过程不同，GNN更关注高阶邻居的信息的聚合。对于时间$t$的每个节点$x$，构建$l$个GNN层来生成：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20230216144129971.png" alt="image-20230216144129971" style="zoom:67%;" />

- **全局结构信息建模**

  全局表示，作为整个图环境的缩写表达，是基于所有节点更新的。在图中，只有少数节点是高活性的，因为它们有大量的相互作用影响图的演化，而大多数长尾节点则容易受到整个图环境的影响。使用全局表示法来填补长尾节点的基本信息，可以保证其表示法更适合于无监督的场景。这里我们在信息传播领域引入LT模型中的节点活跃状态的概念。一个节点的活跃状态随其交互频率而变化，可以用来衡量一个节点在全球环境中的活跃程度。具体来说，节点的活跃状态可用于两部分：（1）控制全局表示的更新；（2）控制全局表示向节点提供信息的权重。

  对于第一部分，全局表示$z_g$在初始化时并不包含任何信息，它需要由节点更新。请注意，在时态图中，节点是按照交互顺序分批训练的。当一批节点被送入模型时，全局表示可以被更新如下：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20230216144329020.png" alt="image-20230216144329020" style="zoom:67%;" />

  在这个公式中，$θ_d$是一个可学习的参数。$|N^t_x|$是节点$x$在时间$t$与之互动的邻居的数量，在这里称之为节点动态。$g^t _g$决定了$x$更新全局表示的程度。一般来说，节点$x$越活跃，它对全局环境的影响就越大，所以它相应的权重$g^t_g$就越大。对于第二部分，全局表征的产生是为了增强长尾节点，因此需要对节点表征进行补充。相反，一个节点越不活跃，它需要全局表示的基本信息就越多。至于活跃度高的节点，它们有大量的交互信息，不需要太多的数据增强。因此，节点表征的更新可以形成如下：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20230216144441425.png" alt="image-20230216144441425" style="zoom:67%;" />

  通过增强长尾节点，模型可以学到更可靠的节点表征。图中的长尾节点越多，效果就越明显，这在下面的实验小节中得到了证明。



### *TGL: A General Framework for Temporal GNN Training on Billion-Scale Graphs

**VLDB 2022**

- TGL，一个用于大规模离线时空图神经网络训练的统一框架，用户可以用简单的配置文件组成各种时空图神经网络。

  TGL包括五个主要部分，一个时态采样器，一个邮箱，一个节点内存模块，一个内存更新器和一个消息传递引擎。我们设计了一个Temporal-CSR数据结构和一个并行采样器，以有效地对时态邻居进行采样，形成训练小批次。

  提出了一种新颖的随机分块调度技术，缓解了大批量训练时节点内存过时的问题。

  为了解决目前的TGNN只在小规模数据集上进行评估的局限性，引入了两个具有2亿和13亿时空边缘的大规模真实世界数据集。评估了TGL在四个小规模数据集上使用单个GPU和两个大数据集上使用多个GPU进行链接预测和节点分类任务的性能。我们将TGL与五种方法的开源代码进行了比较，结果表明TGL以平均13倍的速度实现了类似或更好的准确性。与基线相比，时间并行采样器在多核CPU上实现了平均173倍的速度提升。在4个GPU的机器上，TGL可以在1-10小时内训练一个超过10亿条时空边缘的历时。这是第一个提出在多个GPU上进行大规模时空图神经网络训练的通用框架的工作。

- 本文的主要贡献是

  - 设计了一个统一的框架，通过研究不同的TGNN变体，包括基于快照的TGNN、基于时间编码的TGNN和基于memory的TGNN，支持在大多数TGNN架构上进行高效训练。
  - 设计了一个基于CSR的数据结构，用于快速访问时邻，并设计了一个并行采样器，支持不同的时邻采样算法。
  - 并行采样器可以通过维护辅助指针数组快速定位要采样的时空边缘。
  - 提出了一种新颖的随机分块调度技术，克服了使用节点memory的方法进行大批量训练时对内部依赖性的剥夺，从而实现了大规模动态图上的多GPU训练。
  - 为了更好地比较各种TGNN方法的性能，引入了两个具有数十亿条边的大规模数据集--GDELT和MAG数据集，它们代表了具有长时间的动态图和具有较大节点数的动态图。

- <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20230415143658979.png" alt="image-20230415143658979" style="zoom: 80%;" />

  1.为根节点抽查当前小批中带有时间戳的邻居。

  2.查询内存和支持节点的邮箱。

  3.将输入转移到GPU并更新内存。

  4.使用更新的内存作为输入进行消息传递。

  5.用生成的时态嵌入计算损失。

  6.为下一个小批次处理更新内存和邮箱。

- TGL将具有可学习参数和不可学习参数的模块分开，分别存储在GPU和CPU上。对于GPU内存足以容纳所有信息的数据集，不可学习的模块也可以存储在GPU上并进行计算，以加快训练的速度。

  为了与不同的TGNN变体兼容，设计了五个通用组件：时间采样器、邮箱、节点内存、内存更新器和注意力聚集器。对于基于快照的TGNN，时间采样器将在每个快照中单独采样。请注意，在TGL中，不把图快照当作静态窗口。相反，图快照是根据目标节点的时间戳动态地创建的。这使得基于快照的TGNN能够在任何时间戳上产生动态的节点嵌入，而不是在静态快照中产生恒定的嵌入。

- 在动态图上对邻居进行采样是很复杂的，因为需要考虑邻居的时间戳。在离线训练过程中，TGL静态地存储整个动态图，其中的时间戳被附加到节点和边上。对于基于快照的TGNN，时间采样器需要在采样前识别快照。其他的TGNN要么从所有过去的邻居中均匀采样，要么从最近的邻居中采样，可以被视为具有无限快照长度的单快照TGNN。他们的时间采样器也需要识别候选边和它们的采样概率。因此，设计一个能够快速识别动态候选时空邻域集的数据结构是非常重要的。再加上TGNN训练中的小批次遵循时间顺序（具有非递减的时间）。

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20230415144731036.png" alt="image-20230415144731036" style="zoom: 50%;" />

  节点$𝑣_1$的T-CSR表示，有四个时间边$𝑒_1$到$𝑒_4$，时间戳为1到4，与邻居$𝑣_1$到$𝑣_4$相连。索引和时间数组按照边的时间戳排序，并以边的id $𝑒_1$到$𝑒_4$作为索引。$𝑆_0$和$𝑆_1$表示时间图的两个快照，由指针$𝑝𝑡_0$到$𝑝𝑡_2$指定。

- 采样：在T-CSR数据结构的帮助下，可以快速地在两个指针之间均匀地选择一条边，或者为最近的邻居挑选最接近终点指针的边。这些指针存储在数组中，需要额外的O(𝑛|𝑉  |)存储和O(|𝐸|)计算复杂度来维护一个时代，但允许采样器在O(1)内识别候选边。相比之下，执行二进制搜索将导致在一个历时内识别候选边的计算复杂度为O(|𝐸|  log|𝐸|)。请注意，一些TGNN如TGAT[1]使用邻居的时间戳来采样多跳的时间邻居，而不是使用根节点的时间戳。对于这些TGNN来说，所提出的指针只对第1跳邻居起作用。由于边在T-CSR中被排序，我们仍然可以在采样前使用二进制搜索来快速找出候选边。平行采样  为了利用主机中的多核CPU资源，利用数据的并行性在根节点上进行小批量的采样。

  在每个小批次中，目标节点被均匀地分配给每个线程，以更新指针并对邻居进行采样。请注意，当并行更新指针时，有可能多个线程以不同的时间戳共享相同的目标节点，这将导致竞赛条件。我们为每个节点添加了细粒度的锁，以避免指针在这种条件下被多次推进。当不同时间戳的相同目标节点在一个小批处理中出现多次时，具有小时间戳的目标节点也有可能从未来的时间邻居中采样。在这种情况下，我们通过严格要求采样的时间邻居比根节点的时间戳小来防止信息泄露。在每个线程完成每个minibatch的采样后，为每层生成一个DGL消息流图（MFG），其中包含前向和后向传播中需要的所有输入数据，并将其传递给训练器。



### ***DO WE REALLY NEED COMPLICATED MODEL ARCHITECTURES FOR TEMPORAL NETWORKS?**

ICLR 2023

- 循环神经网络（RNN）和自我注意机制（SAM）是为时间图学习提取空间-时间信息的事实上的方法。有趣的是，本文发现虽然RNN和SAM都能带来良好的性能，但在实践中它们都不是必须的。

  本文提出了GraphMixer，一个概念上和技术上都很简单的架构，由三个部分组成：

  - 一个仅基于多层感知器（MLP）的链接编码器，用于汇总时序链接的信息；

  - 一个仅基于邻居均值池化的节点编码器，用于汇总节点信息；

  - 一个基于MLP的链接分类器，根据编码器的输出进行链接预测。

    尽管GraphMixer很简单，但它在时态链接预测基准上取得了出色的表现，收敛速度更快，泛化性能更好。这些结果促使我们重新思考更简单的模型结构的重要性。

- 导致GraphMixer成功的关键因素是什么？本文认为有三个关键因素促成了GraphMixer的成功： 
  -  GraphMixer的输入数据和神经架构的简单性。与大多数专注于设计概念上复杂的数据准备技术和技术上复杂的神经架构的深度学习方法不同，我们选择简化神经架构并利用概念上更简单的数据作为输入。这两点都能带来更好的模型性能和更好的泛化效果。
  - 一个时间编码函数，将任何时间戳编码为GraphMixer的一个容易区分的输入向量。与大多数建议从原始输入数据中学习时间编码函数的现有方法不同，本文的时间编码函数利用了概念上简单的特征，并且在训练期间是固定的。有趣的是，本文的固定时间编码函数比可训练的版本（被大多数以前的研究使用）更有优势，可以导致更平滑的优化景观，更快的收敛速度和更好的泛化。
  - 一个可以更好地区分时间序列的链接编码器。与大多数使用SAM总结序列的现有方法不同，我们的编码器模块完全基于MLPs。有趣的是，我们的编码器可以区分SAM无法区分的时间序列，并且由于其更简单的神经结构和更低的模型复杂度，它可以更好地进行推广（第4.3节）。

- 本文的贡献总结如下： 
  - 提出了一个概念上和技术上都很简单的架构GraphMixer；
  - 即使没有RNN和SAM，GraphMixer不仅优于所有基线，而且还享有更快的收敛性和更好的泛化能力；
  - 广泛的研究确定了有助于GraphMixer成功的三个因素。
  - 结果可以激励未来的研究，重新思考概念上和技术上更简单的方法的重要性。

- **Link-encoder**：链接编码器旨在总结与每个节点相关的、按时间戳排序的时间链接信息，其中时间链接信息是指每个链接的时间戳和特征。例如在图1中，节点$v_2$的时间链接信息是$x^{link}_{(1,2)}(t_1)$, (t3, xlink 2,4 (t3)), $x^{link}_{(2,4)}(t_4)$, $x^{link}_{(1,2)}(t_5)$。在实践中，只保留前$K$个最新的时间链接信息，其中$K$是一个依赖于数据集的超参数。如果多个链接具有相同的时间戳，只需保持它们与输入的原始数据相同的顺序。为了总结时间链接信息，链接编码器应该有能力区分不同的时间戳（由时间编码函数cos(tω)实现）和不同的时间链接信息（由混合器模块（1-layer MLP-mixer）实现）。

  ![image-20230415173127312](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20230415173127312.png)

- **Node-encoder**：节点编码器被设计成通过邻居平均池捕捉节点身份和节点特征信息。定义节点$v_i$的1跳邻居，其链接时间戳从$t$到$t_0$为$N（v_i;t,t_0）$。例如在图1中，有$N (v_2; t_4, t_0) = {v_1, v_4}$和$N (v_5; t_4, t_0) = {v_3}$。然后，根据1跳邻居计算节点信息特征，即$s_i(t_0) = x^{node}_i + Mean\{x^{node}_j | v_j∈N (v_i; t_0 - T, t_0)\}$，其中$T$是一个依赖数据集的超参数。在实践中，发现1跳邻居足以实现良好的性能，对于没有节点特征的数据集，使用一跳节点表示法。
- **Link classifier**：链接分类器旨在利用链接编码器$t_i(t_0)$的输出和节点编码器$s_i(t_0)$的输出对时间$t_0$是否存在链接进行分类。把节点$v_i$在时间$t_0$的表征表示为上述两个编码的连接$h_i(t_0) = [s_i(t_0) || t_i(t_0)]$。然后，通过对$[h_i(t_0) || h_j(t_0)]$应用2层MLP模型，即$p_{ij} = MLP([h_i(t_0) || h_j(t_0)]$，计算出节点$v_i$、$v_j$之间是否在时间$t_0$发生互动的预测值。

## 时序知识图谱

### *Recurrent Event Network: Autoregressive Structure Inference over Temporal Knowledge Graphs

- 知识图谱推理是自然语言处理中的一项重要任务。这项任务在时序性知识图上变得更加具有挑战性，因为每个事实都与一个时间戳相关。大多数现有的方法都集中在对过去的时间戳进行推理，它们无法预测未来发生的事实。本文提出了循环事件网络（RE-NET），这是一种新型的自回归结构，用于预测未来的交互。一个事件的发生被建模为一个以过去知识图谱的时间序列为条件的概率分布RE-NET采用了一个递归事件编码器来编码过去的事实，并使用邻域聚合器来模拟同一时间戳的事实的联系。然后，未来的事件可以在这两个模块的基础上以顺序的方式推断出来。通过对五个公共数据集的未来时间的链接预测来评估，通过广泛的实验证明了RENET的优势，特别是在未来时间戳的多步骤推断上，在所有五个数据集上实现了最先进的性能。

- 本文方法的关键思想是，从图的序列中学习时间依赖性，从节点的邻域学习局部结构依赖性。全局和局部表示从知识图谱中捕捉到不同方面的知识信息，它们自然是互补的，使我们能够以一种更有效的方式对图谱的生成过程进行建模。

- RE-NET由两部分组成，循环神经网络（RNN）和一个邻域聚合器组成，前者作为循环事件编码器用于表征时间依赖性，后者用于表征图结构依赖性。本文还讨论了RE-NET的参数学习，并通过对中间图的连续采样，为遥远的未来定义了多步骤推理。

  ![image-20221104191943500](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221104191943500.png)

  如上图所示，聚合器对全局图结构和局部邻域进行编码，分别捕捉全局和局部信息。递归事件编码器用图形结构的编码表示序列更新其状态。MLP解码器定义了当前图形的概率。

- 全局表示$H_t$总结了整个图中直到时间戳t的全局信息，它反映了对即将发生的事件的全局偏向。相比之下，局部表示更关注每个主体实体s或每对主体实体和关系（s, r），它捕捉到与这些实体和关系具体相关的知识。将上述局部表征分别表示为$h_t(s)$和$h_t(s, r)$。<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221104193155473.png" alt="image-20221104193155473" style="zoom:67%;" />表示t时刻的图G的有条件分布，它依赖于前m个时刻的图。从该概率分布中产生三联体：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221104193501065.png" alt="image-20221104193501065" style="zoom:80%;" />

  鉴于所有过去的事件$G_{t-m:t-1}$，首先通过$p(s_t|G_{t-m:t-1})$抽取一个主体实体$s_t$，然后通过$p(r_t|s_t, G_{t-m:t-1})$生成一个关系$r_t$，最后通过$p(o_t|s_t, r_t, G_{t-m:t-1})$生成对象实体$o_t$。

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221104193802801.png" alt="image-20221104193802801" style="zoom:80%;" />

  其中<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221104193839320.png" alt="image-20221104193839320" style="zoom:80%;" />是为实体s和关系r生成的可学习的embedding向量。<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221104194006944.png" alt="image-20221104194006944" style="zoom:80%;" />是t-1时刻（s,r）的局部表征。前者可以被理解为是静态的，后者随时间动态变化。在此基础上通过将编码传入多层感知器（MLP）解码器，进一步计算出不同物体实体的概率，MLP解码器定义为一个线性softmax分类器，参数为$\{w_{ot}\}$。

  所有关系和对象的概率定义为：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221104193740404.png" alt="image-20221104193740404" style="zoom:80%;" />

  其中$h_{t-1}(s)$关注的是过去关于s的局部信息，<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221104194800179.png" alt="image-20221104194800179" style="zoom:67%;" />是一个向量表示，用于编码全局图结构$G_{t-m:t-1}$。为了预测一个主体实体将与什么关系互动$p(r_t|s_t, G_{t-m:t-1})$，将静态表示$e_s$以及动态表示$h_{t-1}(s)$视为特征，并将其送入由$w_{r_t}$参数化的多层感知器（MLP）解码器。 此外，为了预测主体实体在时间戳t的分布（即$p(s_t|G_{t-m:t-1})$），将全局表征$H_{t-1}$作为特征，因为它总结了直到时间戳t-1的所有过去图的全局信息，反映了在时间戳t时对即将发生的事件的全局偏向。全局表示$H_t$被期望保留到时间戳t的所有图的全局信息。因此，我们对它们的定义如下：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221104195132691.png" alt="image-20221104195132691" style="zoom:80%;" />

  其中g是聚合函数。

- 本文介绍了三种邻域聚合器：

  ![image-20221104195717462](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221104195717462.png)

  蓝色节点对应的是节点s，红色节点是1跳邻居，绿色节点是2跳邻居。不同颜色的边是不同的关系。Mean和attentive pooling聚合器不区分不同的关系，也不对2跳邻居进行编码，而RGCN聚合器可以纳入多关系和多跳邻居的信息。

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221104195753710.png" alt="image-20221104195753710" style="zoom:67%;" />

### Temporal Knowledge Graph Reasoning Based on Evolutional Representation Learning

- 对不完整的知识图谱进行预测缺失事实的知识图谱（KG）推理已经被广泛探索。然而，对预测未来事实的时间性KG（TKG）的推理还远远没有解决。预测未来事实的关键是要彻底了解历史事实。一个TKG实际上是一个对应于不同时间戳的KG序列，其中每个KG中所有并发的事实都表现出结构上的依赖性，而且时间上相邻的事实都带有信息性的顺序模式。为了有效地捕捉这些特性，我们提出了一种基于图卷积网络（GCN）的新型循环进化网络，称为RE-GCN，它通过对KG序列进行循环建模，在每个时间戳学习实体和关系的进化表示。具体来说，对于演化单元，利用关系感知的GCN来捕捉每个时间戳的KG内的结构依赖。为了捕捉所有并行的事实的顺序模式，历史上的KG序列是由门的递归组件自动建模的。此外，实体的静态属性，如实体类型，也通过静态图约束组件纳入，以获得更好的实体表示。未来时间戳的事实预测可以在进化的实体和关系表征的基础上实现。广泛的实验表明，RE-GCN模型在六个基准数据集的时间推理任务中获得了实质性的性能和效率的提高。特别是，在实体预测方面，它的MRR提高了11.46%，与最先进的baseline相比，速度提高了82倍。
- ![image-20221104201626498](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221104201626498.png)

### Temporal Network Embedding with Micro- and Macro-dynamics

- 网络嵌入的目的是将节点嵌入到一个低维空间，同时捕捉到网络结构和属性。尽管已经提出了不少有前途的网络嵌入方法，但大多数都集中在静态网络上。事实上，时序性网络，通常在微观和宏观动态方面随时间演变，是无处不在的。微观动力学详细描述了网络结构的形成过程，而宏观动力学指的是网络尺度的演变模式。微观动态和宏观动态都是网络演化的关键因素；然而，如何优雅地捕捉这两者的时间网络嵌入，特别是宏观动态，还没有得到很好的研究。
- 本文提出了一种新的具有微观和宏观动态的时态网络嵌入方法，命名为M2DNE。具体来说，对于微观动态，将边缘的建立视为时间事件的发生，并提出了一个时间注意点过程，以精细的方式捕捉网络结构的形成过程。对于宏观动力学，定义了一个以网络嵌入为参数的一般动力学方程，以捕捉内在的演化模式，并在更高的结构层面对网络嵌入施加约束。时态网络中微观和宏观动力学的相互演化会交替影响节点嵌入的学习过程。在三个真实世界的时态网络上进行的大量实验表明，M2DNE不仅在传统任务（如网络重建）中，而且在与时态趋势相关的任务（如规模预测）中都明显优于最先进的技术。

## 图神经网络的分布外泛化鲁棒性

### HANDLING DISTRIBUTION SHIFTS ON GRAPHS: AN INVARIANCE PERSPECTIVE

ICLR 2022

- **分布外泛化：**如何提高在新数据（例如未知分布或未见实体）上的泛化性能是机器学习的一个核心问题。一般的学习问题都是在一个训练集上完成模型训练，而后模型需要在一个新的测试集上给出结果。机器学习问题的误差可以被大致分解为两部分：

  **总误差=表示误差+泛化误差**

  其中表征误差（反映了模型拟合训练数据的能力）是由模型的表达能力/容量决定的，而泛化误差则由在训练集与测试集模型表现的差异决定。当我们采用较为复杂的模型结构（例如神经网络）与有效的优化算法，可以大大降低表征误差。但是当测试数据分布与训练分布呈现明显不同时，模型的泛化误差则很难被控制。这样的场景在实际中也很常见，比如在线下数据进行训练的推荐模型需要泛化到线上的真实场景，在模拟场景下训练的驾驶器要泛化到具有真实交互的环境中。这就是分布外泛化要解决的核心问题：**如何利用有限观测的数据，学习一个稳健的模型，能够泛化到与训练分布有明显差异的测试数据上。**

- **图上的节点级分布外泛化的挑战**

  目前大部分关于分布外泛化问题的研究集中在欧式数据，而对于图结构数据的相关研究还较少。与普通欧式数据不同的是，图结构数据上节点级预测任务的分布偏移问题需要解决两个核心的技术挑战：

  - 样本互连性：由于节点的互连特性，数据样本通常是非独立同分布的，这就为数据分布的建模带来了困难。下图给出了一个简单示意，对于图片数据我们可以把生成每张图片的分布看作相同且独立的；然而对于图结构数据，每个节点的生成依赖于邻居节点，数据分布不能被看作独立的，通常是非独立同分布的，这就为数据生成分布的建模带来了困难。：

    <img src="https://pic3.zhimg.com/80/v2-f8567f384b23223d08170d213138abd6_1440w.webp" alt="img" style="zoom: 50%;" />

  - 图结构信息：除了节点特征外，图的结构也蕴含了重要的信息，会影响到表示学习和预测任务。因此，在考虑数据分布建模与模型泛化的时候，也需要挖掘结构信息的特征并兼顾其影响。

- **本文主要贡献：**

  - 对图上的分布外泛化问题给出了形式化定义，并提供了基于因果不变性假设的分析视角。

  - 从理论上证明了传统学习方法无法实现有效的分布外泛化，并提出了一种新的目标函数（探索-外推风险最小化），用于实现从有限的观测数据向测试分布的外推。

  - 通过理论分析表明新的目标函数可以有效解决分布外泛化问题，并且训练过程可以有效降低测试数据上的泛化误差。

  - 为了验证提出的方法，考虑了三个不同的场景（处理人造混淆噪声、跨图迁移、动态图时序外推），并在多个不同的GNN主干模型上展示了方法的有效性和稳健性。

- **图上的分布偏移与分布外泛化**

  **宏观层面的定义** ：假设输入数据是一个图$G=(A,X)$，它包含了两部分信息: 输入邻接矩阵$A=\{a_{vu}|v,u∈V\}$和节点特征$X=\{x_{v}|v∈V\}$。此外，每个节点对应一个标签，所有节点的标签组成了一个向量$Y=\{y_{v}|v∈V\}$。定义**G**表示输入图的随机变量（是对应的一个具体采样），而**Y**是标签向量的随机变量（同理**Y**是一个具体实现）。此外，引入一个环境变量**e**，它表示与数据生成相关联的某种上下文信息（未被观测到）。于是，图数据的生成过程可以由联合分布的展开进行描述：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221128190820609.png" alt="image-20221128190820609" style="zoom:67%;" />

  然而上述的定义方式不方便对图上的分布外泛化问题进行分析和求解，特别是考虑到图上的节点级任务（此时输入数据通常只有一张图或极少量的图），因此考虑一种微观层面的定义。

  **微观层面的定义** ：将输入的图以节点为单位（通常每个节点就是一个训练/测试样本）分解为一系列子图。具体的，假设$v$表示节点的随机变量，定义节点$v$的$L$阶邻居内的节点集合为$N_v$（这里$L$是任意的正整数）。$N_v$中的节点形成了一个子图$G_v$，它包含了一个（局部）节点特征矩阵$X_v={x_u|u∈N_v}$和一个（局部）邻接矩阵$A_v={a_{uw}|u,w∈N_v}$。同样定义$G_v$为子图的随机变量而$G_v=(A_v,X_v)$是其具体采样。定义$y$是节点标签的随机变量，$y_v$对应具体的采样。由此，将输入图分解为一系列（有重叠）子图的集合$\{(G_v,y_v)\}v∈V$，这里可以将$G_v$视为模型（例如图神经网络）的输入，$y_v$是输出。当$G_v$给定后$v$与图中其他节点可以视为独立的，因此$p(Y|G,e)$可以被分解为$|V|$个独立相同的分布的乘积，即$∏_{v∈V}p(y|G_v,e)$。基于上述定义，可以把观测数据$\{(G_v,y_v)\}v∈V$从数据生成分布$p(G,Y|e)$的采样生成过程看成两步：1)首先采样一个完整的输入图$G∼p(G|e)$，而它可以被视作一系列（有重叠）子图的集合$\{G_v\}_{v∈V}$; 2)接着对图上的每一个单一节点，采样其标签$y∼p(y|$**G_v**$=G_v,e)$。下面给出图上的分布外泛化问题的数学定义：

<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221128191535881.png" alt="image-20221128191535881" style="zoom: 50%;" />

- **基于不变性原理的分布外泛化**

  直接解决上述的问题是非常困难的，因为模型在没有结构性假设和对学习任务的先验知识的情况下往往是不可能实现分布外泛化的。为此，本文从数据生成的角度，通过利用数据背后的因果不变性，来引导模型学习到可以实现泛化的映射关系。

  首先考虑一个具体的例子作为前序铺垫。考虑一个引用网络，每个节点表示一篇论文，每条连边表示论文之间的引用关系。每个节点有两个特征——论文发表的会议$x_1$与论文的影响力$x_2$，标签$y$是论文的主题，环境$e$是论文发表的时间。可以将上述变量的因果关系表示为下图：

  <img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221128192327923.png" alt="image-20221128192327923" style="zoom:67%;" />

  引用网络的例子。图中的三个因果依赖关系可以作如下理解：1）$x_1→y$：论文发表的会议会决定论文研究的主题；2）$y→x_2$：论文的影响力往往与论文的主题有关；3）$e→x_2$：论文的影响力还与论文发表的时间有关（研究方向的流行度会随时间变化）。

  在这个例子中，$x_2$会同时与$y$和e有关。也就是说，当环境发生变化时（对应于数据采样的分布发生了变化），$x_2$与$y$之间的关系也会发生变化。因此，如果模型在训练集上学习到了这部分关联性，当迁移到测试集后就不能获得令人满意的结果（因为环境的改变导致了$x_2$与$y$关系的改变）。相反的，如果模型在训练集中学习到了$x_1$与$y$的关系，就能够成功迁移到测试集（因为就算环境发生了改变，$x_1$与$y$之间的关系是稳定不变的）。这个例子提供的启发是：我们可以引导模型学习与环境无关的关系（具体表现为当环境发生变化时，从$x$到$y$的关系保持不变），就能够帮助其泛化到新的环境中。这就是所谓的**不变性原理（Invariance Principle）**。

  具体的，希望学习一个分类器$y_v=f_θ(G_v)$，它能够从$G_v$中学到相对环境不变的表征$z_v=g(G_v)$，即**z**（表示$z_v$对应的随机变量）需要满足以下两点要求：

  - 环境不变性（Invariance）： 对于任意的环境$e$，分类器给出的预测分布保持不变，即$p(y|z,e)=p(y|z)$ （给定表征$z_v$，环境与预测标签独立）
  - 预测充分性（Sufficiency）：表征包含的信息足够预测标签，即存在一个从$z$到$y$的映射$c∗$, 使得$y=c∗(z)+n$其中$n$是一个随机噪声。

- 受以上思路的启发，本文把学习目标定义为在不同环境上对应风险损失的均值和方差（V是均值，E是方差）：

<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221128193319602.png" alt="image-20221128193319602" style="zoom:50%;" />

​		其中<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221128193334790.png" alt="image-20221128193334790" style="zoom: 33%;" />，$β$是一个权重超参数。这一目标的直观考虑是如果模型在不同的环境下能够给出相近的结果（即loss方差最小		化），其学到的从$x$到$y$的映射就是相对环境不变的。这也有别于传统的监督学习方法Empirical Risk Minimization（ERM），即只对每个样本的loss的均值进行优化，这种情况下模型就很容易学到与环境相关的映射，在训练数据上发生过拟合。

​		然而，上式则要求训练数据中包含来自多个环境的观测数据，并且每个数据样本对应的环境id也是已知的。对于图结构数据，尤其是节点级任务，这两个要求都是不满足的。通常情况下，训练数据只包含了一整张大图，也没有足够的每个节点对应哪个环境的信息。为了解决这一困难，引入$K$个额外的数据生成器		$g_{w_k}(G)(k=1,⋯,K)$，基于输入图生成$K$份不同的图数据$\{G^k\}_{k=1}^K$ 来探索环境，模拟来自不同环境的观测数据。基于此，考虑如下的双层优化学习目标：

<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221128193632364.png" alt="image-20221128193632364" style="zoom: 50%;" />

这里定义每个图数据所对应的损失函数<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221128193704361.png" alt="image-20221128193704361" style="zoom:33%;" />。针对数据生成器$g_{w_k}(G)$，将其参数化为一个图结构编辑器（graph editor），即将每一条连边假设为自由参数，对输入图进行局部改变（删除或增加连边）。具体的，将每一个改变视为动作（action），最终使用基于策略梯度的REINFORCE算法进行优化，以解决离散动作空间采样不可导的问题。我们将本文提出的方法称为Explore-to-Extrapolation Risk Minimization（EERM)，下图给出了训练过程的数据流图：

<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221128193812453.png" alt="image-20221128193812453" style="zoom:67%;" />

### IN-DISTRIBUTION AND OUT-OF-DISTRIBUTION GENERALIZATION FOR GRAPH NEURAL NETWORKS

ICLR 2023

- 图形神经网络（GNNs）是允许用不同大小的结构化数据进行学习的模型。尽管GNNs很受欢迎，但对GNNs泛化的理论理解是一个未被充分探索的话题。本文扩展了对GNNs分布内和分布外泛化的理论理解。首先，改进了最先进的PAC-Bayes（分布内）泛化约束，主要是将对节点度的指数依赖性减少到线性依赖性。其次，利用谱图理论的工具，证明了一些关于GNN的分布外（OOD）size泛化的严格保证，其中训练集的图与测试集的图具有不同的节点和边的数量。为了从经验上验证理论发现，在合成图和真实世界图数据集上进行了实验。计算出的分布内情况下的泛化差距明显改善了最先进的PAC-Bayes结果。对于OOD情况，在大型社交网络中的社区分类任务的实验表明，在该理论保证的情况下，GNNs实现了强大的规模泛化性能。

- 根据问题设置是分布内（ID）还是分布外（OOD），即测试数据是否与训练数据来自同一分布，将相关工作分为两组：ID Generalization和OOD Generalization。之前的相关工作都遵循不变风险最小化（invariant risk minimization）的思想，并专注于设计新的学习目标。相反地，本文从传统的统计学习理论角度提供泛化约束分析。

- 本文研究了GNNs的分布内和分布外泛化。对于分布内的图分类任务，通过将对最大节点度的指数依赖性降低到线性依赖性，大大改善了之前最先进的PAC-Bayes结果。对于OOD节点分类任务，本文方法不假设任何已知的图生成模型，这与现有工作形成鲜明对比。相反，假设GNN在子图上进行训练和测试，这些子图是通过随机行走从一个大的底层图中取样的，是产生连接子图的一种有效手段。本文确定了一些有趣的情况，在这些情况下，理论上保证了图分类任务在规模泛化方面表现良好，并推导出泛化的界限。通过在合成图上进行实验来验证本文的理论结果，同时也在现实世界的社会网络数据集上探索大小泛化。在分布内的情况下，观察到泛化边界的数值计算有几个数量级的改进。在分布外的情况下，验证了在理论保证尺寸泛化工作良好的情况下，大子图的预测精度总是与小子图的精度相当，而且在许多情况下实际上是更好。

- ![image-20221201150111392](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221201150111392.png)

  

上图a是一个小扩张器图的例子，其节点的任何标记都不能表现出同质性。b是小杠铃图的例子。如果一个标签在两组之间有确切的区分，那么它就表现出同质性。

- 内分布PAC-Bayes边界的改进：先前的工作使用PAC-Bayes理论制定了内分布情况下GNN的最先进的泛化边界。具体来说，他们建立在PAC-Bayes定理之上，该定理涉及到同质前馈神经网络。将一个样本表示为<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221201150508190.png" alt="image-20221201150508190" style="zoom:67%;" />，其中<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221201150459895.png" alt="image-20221201150459895" style="zoom:67%;" />分别是节点特征、邻接矩阵和图标签。每个样本都是以i.i.d.的方式从一些未知的数据分布$D$（支持的<img src="C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221201150933956.png" alt="image-20221201150933956" style="zoom:67%;" />)中抽取的。由于训练和测试样本都来自同一个分布，这就是分布内设置。本文考虑多类图分类的margin loss如下：

  ![image-20221201150610417](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221201150610417.png)

  其中γ >  0是边的参数，$f_w$是由权重$w$参数化的模型。由于数据分布$D$是未知的，无法计算真实的损失值，本文最小化经验损失（风险），该经验损失在采样训练集$S$上定义如下：

  ![image-20221201151004560](C:\Users\Funny\AppData\Roaming\Typora\typora-user-images\image-20221201151004560.png)

  其中$m$是训练样本的数量。

- 本文为GNNs开发了一个分布外（OOD）泛化理论。由于采用的是统计学习的观点，所以必须有一些与训练图和测试图相关的假设，在与实际相关的假设和那些可以证明严格保证的假设之间有一个权衡。选择了一些认为在这些目标之间取得平衡的假设，至少对于像社交网络这样的应用来说是这样。
  - 尺寸泛化假设：考虑以下设置，首先假设存在一个极其庞大的图G，如Twitter中的用户网络，因此人们需要对子图进行抽样（例如，通过随机漫步），以训练和测试机器学习模型。为了生成训练和测试子图，在这个单一的大图上分别运行长度为N和M的随机行走，其中M≫N，并收集这些行走所引起的子图。然后在较短（长度为N）的行走所引起的子图上训练GNN。在测试中，假设一个程序，即从大子图中抽出一个长度为M的随机行走诱导的子图。随机行走是通过从图中的所有节点中均匀地随机选择一个初始节点开始的，每一步都有相同的概率选择当前节点的任何邻居。这是一个有趣的OOD问题，训练图和测试图来自不同的分布，由底层大图和具有特定长度的随机行走采样决定。考虑图的分类问题，并假设图的标签是由图中的大多数节点标签决定的，这对许多涉及同质图的应用是合理的。对于节点标签，假设它是二进制的，但对标签的生成方式没有假设。最重要的是，对底层大图没有任何假设。因此，我们的设置比文献中的一些OOD设置有优势，在这些设置中明确假设了图和标签的生成模型。
